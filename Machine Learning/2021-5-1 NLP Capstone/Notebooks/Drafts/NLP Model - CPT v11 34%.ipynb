{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "built-romance",
   "metadata": {},
   "source": [
    "# Changes from v10\n",
    "* Using sentence tokenize as an initial pass in order to try and filter to the most pertinent information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-alabama",
   "metadata": {},
   "source": [
    "\n",
    "# Import the MIMIC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dominant-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (4,5,7,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "dataset_dictionary = {}\n",
    "\n",
    "for file_path in glob.glob('.\\\\Data\\\\MIMIC Files\\*'):\n",
    "    file_name = file_path.split('\\\\')[3].split('.')[0]\n",
    "    with gzip.open(file_path, mode='r') as file:\n",
    "        dataset_dictionary[file_name] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-desert",
   "metadata": {},
   "source": [
    "# Join the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "grateful-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset to join together -----\n",
    "\n",
    "# Create note_events table -----\n",
    "\n",
    "# Combine text for each subject and encounter\n",
    "note_events_base = dataset_dictionary['NOTEEVENTS'][dataset_dictionary['NOTEEVENTS'].loc[:,'CATEGORY'] == 'Discharge summary']\n",
    "note_events = note_events_base.groupby(['SUBJECT_ID', 'HADM_ID'], as_index=False)['TEXT'].agg(sum)\n",
    "\n",
    "# Create CPT table -----\n",
    "\n",
    "cpt_events_base = dataset_dictionary['CPTEVENTS']\n",
    "cpt_events_base = cpt_events_base[cpt_events_base['TICKET_ID_SEQ'] == 1]\n",
    "cpt_events_base = cpt_events_base.loc[:, ['SUBJECT_ID','HADM_ID', 'CPT_CD']]\n",
    "cpt_events = cpt_events_base.drop_duplicates()\n",
    "cpt_events\n",
    "\n",
    "# Join the datasets -----\n",
    "\n",
    "note_cpt = note_events.merge(cpt_events, on = ['SUBJECT_ID','HADM_ID'])\n",
    "# print(note_cpt.shape, note_events.shape, cpt_events.shape) # (223,150, 4) (52,726, 3) (227,510, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-dealing",
   "metadata": {},
   "source": [
    "# Filter the data to CPT over 200 samples + Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "pursuant-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99291    7860\n",
      "99223    2851\n",
      "99222    1736\n",
      "99254    1242\n",
      "99255     882\n",
      "         ... \n",
      "62272       1\n",
      "50547       1\n",
      "53215       1\n",
      "39561       1\n",
      "63082       1\n",
      "Name: CPT_CD, Length: 707, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Value Counts\n",
    "print(note_cpt['CPT_CD'].astype(str).value_counts())\n",
    "\n",
    "# Filter to CPT with over 200 notes\n",
    "df = note_cpt['CPT_CD'].astype(str).value_counts()\n",
    "top_200 = list((df[df > 200]).index.values)\n",
    "note_cpt_4 = note_cpt[note_cpt['CPT_CD'].astype(str).isin(top_200)]\n",
    "\n",
    "# Resample minority groups -----\n",
    "\n",
    "# Remove largest group\n",
    "top_200.remove('99291')\n",
    "'99291' in top_200\n",
    "\n",
    "# minority_ls = top_200\n",
    "minority_ls = ['99223','99222','99254']\n",
    "\n",
    "minority_df = []\n",
    "for i in minority_ls:\n",
    "    test_resampled = resample(note_cpt[note_cpt['CPT_CD'].astype(str) == i], replace=True, n_samples=7860, random_state=123)\n",
    "    minority_df.append(test_resampled)\n",
    "\n",
    "minority_df.append(note_cpt[note_cpt['CPT_CD'].astype(str) == '99291'])\n",
    "new_df = pd.concat(minority_df)\n",
    "\n",
    "new_df['CPT_CD'] = new_df['CPT_CD'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-steering",
   "metadata": {},
   "source": [
    "# Check for Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "altered-lucas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99291    7860\n",
       "99222    7860\n",
       "99254    7860\n",
       "99223    7860\n",
       "Name: CPT_CD, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(note_cpt['CPT_CD'].astype(str))\n",
    "# plt.show()\n",
    "\n",
    "new_df['CPT_CD'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-approach",
   "metadata": {},
   "source": [
    "# Sentence tokenizer to restructure dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "british-calcium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPT_CD</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99223</td>\n",
       "      <td>['Admission Date:  [**2155-4-15**]            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99223</td>\n",
       "      <td>['Admission Date:  [**2173-5-7**]             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99223</td>\n",
       "      <td>['Admission Date:  [**2176-9-22**]            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99223</td>\n",
       "      <td>['Admission Date:  [**2173-5-18**]            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99223</td>\n",
       "      <td>['Admission Date:  [**2116-5-14**]            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19076261</th>\n",
       "      <td>99291</td>\n",
       "      <td>'The office number is [**Telephone/Fax (1) 49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19107701</th>\n",
       "      <td>99291</td>\n",
       "      <td>'2.'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19139141</th>\n",
       "      <td>99291</td>\n",
       "      <td>'Follow up appointment with Dr. [**First Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19170581</th>\n",
       "      <td>99291</td>\n",
       "      <td>'Office phone number is [**Telephone/Fax (1) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19202021</th>\n",
       "      <td>99291</td>\n",
       "      <td>'[**First Name11 (Name Pattern1) **] [**Last ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4053237 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CPT_CD                                               TEXT\n",
       "0         99223  ['Admission Date:  [**2155-4-15**]            ...\n",
       "1         99223  ['Admission Date:  [**2173-5-7**]             ...\n",
       "2         99223  ['Admission Date:  [**2176-9-22**]            ...\n",
       "3         99223  ['Admission Date:  [**2173-5-18**]            ...\n",
       "4         99223  ['Admission Date:  [**2116-5-14**]            ...\n",
       "...         ...                                                ...\n",
       "19076261  99291   'The office number is [**Telephone/Fax (1) 49...\n",
       "19107701  99291                                               '2.'\n",
       "19139141  99291   'Follow up appointment with Dr. [**First Name...\n",
       "19170581  99291   'Office phone number is [**Telephone/Fax (1) ...\n",
       "19202021  99291   '[**First Name11 (Name Pattern1) **] [**Last ...\n",
       "\n",
       "[4053237 rows x 2 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# pd.concat([test['column 1'].str.split(',', expand=True), test['predictive column']], axis=1).melt(id_vars='predictive column').drop('variable', axis=1)\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/33098040/how-to-use-word-tokenize-in-data-frame\n",
    "\n",
    "def expand_dataframe_by_sent(dataframe, text_col_name, pred_col_name):\n",
    "    \n",
    "    # # Step 1: Remove all commas\n",
    "    dataframe[text_col_name] = dataframe[text_col_name].str.replace(',',' ')\n",
    "\n",
    "    # # Step 2: Sentence tokenize and convert into a large string\n",
    "    dataframe[text_col_name] = dataframe.apply(lambda row: sent_tokenize(row[text_col_name]), axis=1)\n",
    "\n",
    "    # # Step 3 + 4: Split into columns & concatenate with original data\n",
    "    updated_df = pd.concat([dataframe[text_col_name].astype(str).str.split(',', expand=True), dataframe[pred_col_name]], axis=1).melt(id_vars=pred_col_name, value_name=text_col_name).drop('variable', axis=1)\n",
    "    \n",
    "    updated_df.dropna(inplace=True)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "new_df_1 = expand_dataframe_by_sent(new_df, 'TEXT', 'CPT_CD')\n",
    "new_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-september",
   "metadata": {},
   "source": [
    "# Filter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "unable-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_data(text_series):\n",
    "    \n",
    "    # Replace \\n \n",
    "    text_series = text_series.str.replace('\\\\n',' ', regex=True)    \n",
    "\n",
    "    # Remove dates and locations\n",
    "    text_series = text_series.str.replace('\\[\\*\\*(.*?)\\*\\*\\]', ' ', regex=True)\n",
    "    \n",
    "#     # Remove topics\n",
    "#     data = text_series.str.split('([A-Z\\s]+:)')\n",
    "#     for row_num, value in enumerate(data):\n",
    "#         text_chunks = [x.strip().replace(':','').replace('\\n', '') for x in value]\n",
    "#         for i, x in enumerate(text_chunks):\n",
    "#             if 'MEDICATION' in x or 'SOCIAL HISTORY' in x or 'FAMILY HISTORY' in x:\n",
    "#                 text_chunks[i] = ' '\n",
    "#                 try:\n",
    "#                     text_chunks[i + 1] = ' '\n",
    "#                 except:\n",
    "#                     continue\n",
    "\n",
    "#         text_series.iloc[row_num] = ' '.join(text_chunks)\n",
    "    \n",
    "    # Replace punctuation\n",
    "    text_series = text_series.str.replace('[' + string.punctuation + ']', ' ', regex=True)\n",
    "    \n",
    "    # Convert to lowercase \n",
    "    text_series = text_series.str.lower()\n",
    "    \n",
    "    # Remove all digits\n",
    "    text_series = text_series.str.replace('\\d',' ', regex=True)\n",
    "    \n",
    "    # Replace plurals, endings with ing, endings with ed, endings with ly\n",
    "    text_series = text_series.str.replace('s(?=\\s)', ' ', regex=True)\n",
    "    text_series = text_series.str.replace('ing(?=\\s)', ' ', regex=True)\n",
    "    text_series = text_series.str.replace('ed(?=\\s)', ' ', regex=True)\n",
    "    text_series = text_series.str.replace('ly(?=\\s)', ' ', regex=True)\n",
    "    \n",
    "    return text_series\n",
    "\n",
    "# Update Text Column\n",
    "\n",
    "new_df_1.loc[:, 'TEXT'] = clean_data(new_df_1['TEXT']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-bracket",
   "metadata": {},
   "source": [
    "# Shuffle the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "surprised-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_1 = new_df_1.sample(n = len(new_df_1), random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-devil",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "extensive-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages -----\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "my_stop_words = list(set(stopwords.words('english'))) \\\n",
    "                + ['admission', 'date', 'sex'] \\\n",
    "                + ['needed', 'every', 'seen', 'weeks', 'please', 'ml', 'unit', 'small', 'year', 'old', 'cm', 'non', 'mm', 'however']\n",
    "                # Got the above from my top 100 most predictive words that I wanted to remove\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split the data -----\n",
    "\n",
    "X_train, X_test, y_train, y_test, index_train, index_test = train_test_split(new_df_1['TEXT'].values, new_df_1['CPT_CD'].astype(str), range(len(new_df_1['CPT_CD'])), test_size = .8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-comment",
   "metadata": {},
   "source": [
    "# Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "determined-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data -----\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=my_stop_words, min_df = 3, max_df = .7, sublinear_tf=True)\n",
    "\n",
    "# Transform the training data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-grenada",
   "metadata": {},
   "source": [
    "# Run Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cultural-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Naive Bayes model -----\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB(alpha=.7)\n",
    "\n",
    "# Fit and check accuracy\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "pred = nb_classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-reform",
   "metadata": {},
   "source": [
    "# Remove Rows That Didn't Predict Correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "received-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = []\n",
    "\n",
    "# Make a list of indices in y_test that contain the misclassified images\n",
    "for index, value in enumerate(pred):\n",
    "    if value != y_test.values[index]:\n",
    "        misclassified.append(index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_df_2 = new_df_1.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "directed-appearance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 14,\n",
       " 15,\n",
       " 17,\n",
       " 18,\n",
       " 22,\n",
       " 24,\n",
       " 25,\n",
       " 27,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 35,\n",
       " 36,\n",
       " 38,\n",
       " 40,\n",
       " 41,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 54,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 69,\n",
       " 71,\n",
       " 74,\n",
       " 75,\n",
       " 77,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 89,\n",
       " 93,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 113,\n",
       " 114,\n",
       " 119,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 140,\n",
       " 141,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 156,\n",
       " 157,\n",
       " 159,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 166,\n",
       " 167,\n",
       " 169,\n",
       " 172,\n",
       " 173,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 184,\n",
       " 185,\n",
       " 187,\n",
       " 189,\n",
       " 190,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 220,\n",
       " 221,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 237,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 243,\n",
       " 244,\n",
       " 246,\n",
       " 247,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 255,\n",
       " 256,\n",
       " 259,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 266,\n",
       " 268,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 282,\n",
       " 283,\n",
       " 286,\n",
       " 288,\n",
       " 290,\n",
       " 291,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 302,\n",
       " 305,\n",
       " 306,\n",
       " 310,\n",
       " 311,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 327,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 334,\n",
       " 337,\n",
       " 338,\n",
       " 342,\n",
       " 343,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 365,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 371,\n",
       " 372,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 379,\n",
       " 383,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 408,\n",
       " 410,\n",
       " 411,\n",
       " 415,\n",
       " 416,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 448,\n",
       " 450,\n",
       " 451,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 465,\n",
       " 467,\n",
       " 469,\n",
       " 470,\n",
       " 473,\n",
       " 479,\n",
       " 482,\n",
       " 483,\n",
       " 485,\n",
       " 486,\n",
       " 488,\n",
       " 489,\n",
       " 492,\n",
       " 495,\n",
       " 496,\n",
       " 498,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 504,\n",
       " 506,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 516,\n",
       " 517,\n",
       " 520,\n",
       " 522,\n",
       " 524,\n",
       " 526,\n",
       " 528,\n",
       " 531,\n",
       " 533,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 540,\n",
       " 541,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 558,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 568,\n",
       " 570,\n",
       " 572,\n",
       " 577,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 584,\n",
       " 585,\n",
       " 587,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 598,\n",
       " 599,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 612,\n",
       " 614,\n",
       " 616,\n",
       " 617,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 627,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 653,\n",
       " 654,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 691,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 715,\n",
       " 717,\n",
       " 719,\n",
       " 721,\n",
       " 723,\n",
       " 724,\n",
       " 730,\n",
       " 731,\n",
       " 733,\n",
       " 735,\n",
       " 736,\n",
       " 740,\n",
       " 741,\n",
       " 744,\n",
       " 746,\n",
       " 747,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 759,\n",
       " 760,\n",
       " 763,\n",
       " 764,\n",
       " 767,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 789,\n",
       " 793,\n",
       " 794,\n",
       " 796,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 811,\n",
       " 813,\n",
       " 816,\n",
       " 819,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 825,\n",
       " 826,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 842,\n",
       " 844,\n",
       " 845,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 861,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 869,\n",
       " 870,\n",
       " 872,\n",
       " 875,\n",
       " 876,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 884,\n",
       " 886,\n",
       " 888,\n",
       " 889,\n",
       " 893,\n",
       " 900,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 910,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 931,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 945,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 957,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 981,\n",
       " 982,\n",
       " 984,\n",
       " 985,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 995,\n",
       " 996,\n",
       " 999,\n",
       " 1000,\n",
       " 1001,\n",
       " 1003,\n",
       " 1004,\n",
       " 1005,\n",
       " 1006,\n",
       " 1007,\n",
       " 1008,\n",
       " 1009,\n",
       " 1010,\n",
       " 1012,\n",
       " 1013,\n",
       " 1014,\n",
       " 1015,\n",
       " 1017,\n",
       " 1018,\n",
       " 1019,\n",
       " 1020,\n",
       " 1021,\n",
       " 1022,\n",
       " 1024,\n",
       " 1025,\n",
       " 1026,\n",
       " 1027,\n",
       " 1028,\n",
       " 1029,\n",
       " 1030,\n",
       " 1034,\n",
       " 1035,\n",
       " 1036,\n",
       " 1038,\n",
       " 1040,\n",
       " 1042,\n",
       " 1043,\n",
       " 1045,\n",
       " 1046,\n",
       " 1048,\n",
       " 1050,\n",
       " 1051,\n",
       " 1052,\n",
       " 1055,\n",
       " 1056,\n",
       " 1057,\n",
       " 1058,\n",
       " 1059,\n",
       " 1060,\n",
       " 1064,\n",
       " 1065,\n",
       " 1068,\n",
       " 1070,\n",
       " 1071,\n",
       " 1073,\n",
       " 1074,\n",
       " 1075,\n",
       " 1076,\n",
       " 1078,\n",
       " 1079,\n",
       " 1080,\n",
       " 1081,\n",
       " 1085,\n",
       " 1087,\n",
       " 1088,\n",
       " 1089,\n",
       " 1090,\n",
       " 1091,\n",
       " 1092,\n",
       " 1093,\n",
       " 1095,\n",
       " 1096,\n",
       " 1099,\n",
       " 1101,\n",
       " 1102,\n",
       " 1103,\n",
       " 1105,\n",
       " 1107,\n",
       " 1109,\n",
       " 1111,\n",
       " 1112,\n",
       " 1113,\n",
       " 1114,\n",
       " 1115,\n",
       " 1116,\n",
       " 1118,\n",
       " 1119,\n",
       " 1120,\n",
       " 1121,\n",
       " 1122,\n",
       " 1123,\n",
       " 1126,\n",
       " 1128,\n",
       " 1129,\n",
       " 1131,\n",
       " 1132,\n",
       " 1135,\n",
       " 1136,\n",
       " 1138,\n",
       " 1139,\n",
       " 1141,\n",
       " 1142,\n",
       " 1144,\n",
       " 1145,\n",
       " 1146,\n",
       " 1147,\n",
       " 1151,\n",
       " 1152,\n",
       " 1153,\n",
       " 1155,\n",
       " 1156,\n",
       " 1157,\n",
       " 1160,\n",
       " 1161,\n",
       " 1164,\n",
       " 1167,\n",
       " 1168,\n",
       " 1169,\n",
       " 1171,\n",
       " 1172,\n",
       " 1176,\n",
       " 1179,\n",
       " 1181,\n",
       " 1182,\n",
       " 1183,\n",
       " 1184,\n",
       " 1186,\n",
       " 1187,\n",
       " 1188,\n",
       " 1189,\n",
       " 1192,\n",
       " 1193,\n",
       " 1194,\n",
       " 1196,\n",
       " 1197,\n",
       " 1199,\n",
       " 1200,\n",
       " 1201,\n",
       " 1202,\n",
       " 1203,\n",
       " 1205,\n",
       " 1206,\n",
       " 1207,\n",
       " 1209,\n",
       " 1211,\n",
       " 1212,\n",
       " 1213,\n",
       " 1217,\n",
       " 1218,\n",
       " 1222,\n",
       " 1223,\n",
       " 1224,\n",
       " 1225,\n",
       " 1226,\n",
       " 1227,\n",
       " 1229,\n",
       " 1230,\n",
       " 1232,\n",
       " 1234,\n",
       " 1236,\n",
       " 1237,\n",
       " 1239,\n",
       " 1241,\n",
       " 1242,\n",
       " 1243,\n",
       " 1244,\n",
       " 1245,\n",
       " 1246,\n",
       " 1247,\n",
       " 1248,\n",
       " 1249,\n",
       " 1254,\n",
       " 1255,\n",
       " 1256,\n",
       " 1257,\n",
       " 1259,\n",
       " 1262,\n",
       " 1263,\n",
       " 1266,\n",
       " 1267,\n",
       " 1268,\n",
       " 1271,\n",
       " 1272,\n",
       " 1273,\n",
       " 1274,\n",
       " 1276,\n",
       " 1277,\n",
       " 1279,\n",
       " 1280,\n",
       " 1281,\n",
       " 1284,\n",
       " 1285,\n",
       " 1287,\n",
       " 1291,\n",
       " 1294,\n",
       " 1296,\n",
       " 1297,\n",
       " 1299,\n",
       " 1300,\n",
       " 1301,\n",
       " 1302,\n",
       " 1308,\n",
       " 1310,\n",
       " 1311,\n",
       " 1312,\n",
       " 1317,\n",
       " 1318,\n",
       " 1319,\n",
       " 1320,\n",
       " 1321,\n",
       " 1322,\n",
       " 1323,\n",
       " 1326,\n",
       " 1332,\n",
       " 1333,\n",
       " 1334,\n",
       " 1335,\n",
       " 1337,\n",
       " 1338,\n",
       " 1340,\n",
       " 1343,\n",
       " 1344,\n",
       " 1345,\n",
       " 1347,\n",
       " 1348,\n",
       " 1350,\n",
       " 1351,\n",
       " 1352,\n",
       " 1353,\n",
       " 1354,\n",
       " 1355,\n",
       " 1356,\n",
       " 1357,\n",
       " 1358,\n",
       " 1360,\n",
       " 1361,\n",
       " 1363,\n",
       " 1364,\n",
       " 1366,\n",
       " 1369,\n",
       " 1370,\n",
       " 1371,\n",
       " 1372,\n",
       " 1373,\n",
       " 1375,\n",
       " 1377,\n",
       " 1378,\n",
       " 1380,\n",
       " 1381,\n",
       " 1382,\n",
       " 1385,\n",
       " 1386,\n",
       " 1387,\n",
       " 1388,\n",
       " 1390,\n",
       " 1391,\n",
       " 1392,\n",
       " 1393,\n",
       " 1394,\n",
       " 1396,\n",
       " 1399,\n",
       " 1401,\n",
       " 1402,\n",
       " 1403,\n",
       " 1408,\n",
       " 1410,\n",
       " 1414,\n",
       " 1415,\n",
       " 1416,\n",
       " 1418,\n",
       " 1419,\n",
       " 1420,\n",
       " 1421,\n",
       " 1423,\n",
       " 1424,\n",
       " 1426,\n",
       " 1427,\n",
       " 1428,\n",
       " 1429,\n",
       " 1432,\n",
       " 1433,\n",
       " 1434,\n",
       " 1436,\n",
       " 1440,\n",
       " 1442,\n",
       " 1445,\n",
       " 1451,\n",
       " 1455,\n",
       " 1458,\n",
       " 1459,\n",
       " 1462,\n",
       " 1463,\n",
       " 1465,\n",
       " 1466,\n",
       " 1467,\n",
       " 1468,\n",
       " 1471,\n",
       " 1473,\n",
       " 1476,\n",
       " 1477,\n",
       " 1479,\n",
       " 1480,\n",
       " 1481,\n",
       " 1482,\n",
       " 1486,\n",
       " 1487,\n",
       " 1488,\n",
       " 1489,\n",
       " 1490,\n",
       " 1491,\n",
       " 1495,\n",
       " 1497,\n",
       " 1499,\n",
       " 1500,\n",
       " 1501,\n",
       " 1502,\n",
       " 1504,\n",
       " 1505,\n",
       " 1506,\n",
       " 1507,\n",
       " 1508,\n",
       " 1509,\n",
       " 1510,\n",
       " 1511,\n",
       " 1513,\n",
       " 1514,\n",
       " 1515,\n",
       " 1516,\n",
       " 1517,\n",
       " 1518,\n",
       " 1519,\n",
       " 1520,\n",
       " 1521,\n",
       " 1523,\n",
       " 1526,\n",
       " 1527,\n",
       " 1528,\n",
       " 1529,\n",
       " 1530,\n",
       " 1532,\n",
       " 1533,\n",
       " 1534,\n",
       " 1537,\n",
       " 1538,\n",
       " 1543,\n",
       " 1545,\n",
       " 1546,\n",
       " 1547,\n",
       " 1548,\n",
       " 1549,\n",
       " 1550,\n",
       " 1554,\n",
       " 1556,\n",
       " 1557,\n",
       " 1558,\n",
       " 1561,\n",
       " 1563,\n",
       " 1564,\n",
       " 1565,\n",
       " 1568,\n",
       " 1570,\n",
       " 1571,\n",
       " 1572,\n",
       " 1573,\n",
       " 1574,\n",
       " 1579,\n",
       " 1580,\n",
       " 1581,\n",
       " ...]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # misclassified\n",
    "drop_list = []\n",
    "\n",
    "for i in misclassified:\n",
    "    drop_list.append(index_test[i])\n",
    "\n",
    "# new_df_3 = new_df_2.reindex(index_test)\n",
    "\n",
    "drop_list\n",
    "\n",
    "count = 0\n",
    "# new_df_2.drop([2903707], axis=0)\n",
    "for i in drop_list:\n",
    "    try:\n",
    "        new_df_1.drop(i, axis=0)\n",
    "        count += 1\n",
    "    except:\n",
    "        continue\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "convenient-religion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CPT_CD</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3829675</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555380</th>\n",
       "      <td>99254</td>\n",
       "      <td>hypercholesterolemia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CPT_CD                      TEXT\n",
       "3829675    NaN                       NaN\n",
       "555380   99254    hypercholesterolemia  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_2.reindex([3829675, 555380])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-decrease",
   "metadata": {},
   "source": [
    "# Tune NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sexual-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.421076621541547\n",
      "0.1\n",
      "0.421076621541547\n",
      "0.2\n",
      "0.421076621541547\n",
      "0.30000000000000004\n",
      "0.421076621541547\n",
      "0.4\n",
      "0.421076621541547\n",
      "0.5\n",
      "0.421076621541547\n",
      "0.6000000000000001\n",
      "0.421076621541547\n",
      "0.7000000000000001\n",
      "0.421076621541547\n",
      "0.8\n",
      "0.421076621541547\n",
      "0.9\n",
      "0.421076621541547\n",
      "1.0\n",
      "0.421076621541547\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def hyperparam_tuning(tfidf_train, y_train, tfidf_test, y_test, nb_classifier):\n",
    "    for i in np.arange(0,1.1,.1):\n",
    "        nb_classifier = MultinomialNB()\n",
    "        nb_classifier.fit(tfidf_train, y_train)\n",
    "        pred = nb_classifier.predict(tfidf_test)\n",
    "        print(i)\n",
    "        print(metrics.accuracy_score(y_test, pred))\n",
    "\n",
    "hyperparam_tuning(tfidf_train, y_train, tfidf_test, y_test, nb_classifier)  \n",
    "\n",
    "# Looks like .6-.7 are the best alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-guess",
   "metadata": {},
   "source": [
    "# Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "under-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_logist = LogisticRegression(C=.001, random_state = 42, multi_class = 'multinomial', penalty='l2')\n",
    "clf_logist.fit(tfidf_train, y_train)\n",
    "logist_pred = clf_logist.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-ultimate",
   "metadata": {},
   "source": [
    "# Looking at Feature Names and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "decent-lotus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['citizenship',\n",
       " 'inutbag',\n",
       " 'reamin',\n",
       " 'taspe',\n",
       " 'traycan',\n",
       " 'eleveat',\n",
       " 'interscapular',\n",
       " 'stenossis',\n",
       " 'spp',\n",
       " 'tct',\n",
       " 'resectable',\n",
       " 'throacotomy',\n",
       " 'evisiting',\n",
       " 'nannulu',\n",
       " 'tplt',\n",
       " 'arthrotec',\n",
       " 'asberger',\n",
       " 'nnor',\n",
       " 'sttw',\n",
       " 'inconssitent',\n",
       " 'contemplate',\n",
       " 'whofelt',\n",
       " 'nreporttemporal',\n",
       " 'femaile',\n",
       " 'nbengay',\n",
       " 'synoviti',\n",
       " 'flie',\n",
       " 'controls',\n",
       " 'navalide',\n",
       " 'nlipomatosi',\n",
       " 'endeavour',\n",
       " 'tractor',\n",
       " 'expectorated',\n",
       " 'tegretal',\n",
       " 'nvalproate',\n",
       " 'walnut',\n",
       " 'ncarie',\n",
       " 'fulminan',\n",
       " 'nrole',\n",
       " 'shariro',\n",
       " 'duckbill',\n",
       " 'nviridan',\n",
       " 'supression',\n",
       " 'mwt',\n",
       " 'pseudobulbar',\n",
       " 'propecia',\n",
       " 'nqpm',\n",
       " 'kindey',\n",
       " 'narrowed',\n",
       " 'maex',\n",
       " 'inheritable',\n",
       " 'speculum',\n",
       " 'intracran',\n",
       " 'otsc',\n",
       " 'chornical',\n",
       " 'nvetriculography',\n",
       " 'nhycodan',\n",
       " 'abcces',\n",
       " 'hypotenion',\n",
       " 'blank',\n",
       " 'nfile',\n",
       " 'herniorrhaphie',\n",
       " 'nklonipin',\n",
       " 'ntelapivir',\n",
       " 'sfebrile',\n",
       " 'nafld',\n",
       " 'nkiss',\n",
       " 'falumouth',\n",
       " 'hematoemesi',\n",
       " 'wrote',\n",
       " 'nspecgr',\n",
       " 'npostoop',\n",
       " 'repond',\n",
       " 'nnwh',\n",
       " 'reliever',\n",
       " 'nresultant',\n",
       " 'nmeasures',\n",
       " 'digibind',\n",
       " 'nankles',\n",
       " 'nconclusive',\n",
       " 'fse',\n",
       " 'disinhibition',\n",
       " 'thoughts',\n",
       " 'grzymish',\n",
       " 'neoformans',\n",
       " 'profolol',\n",
       " 'bloodly',\n",
       " 'inspired',\n",
       " 'dialyze',\n",
       " 'ncontiguou',\n",
       " 'transspatial',\n",
       " 'nrestroom',\n",
       " 'ninstillation',\n",
       " 'nsatisfi',\n",
       " 'sudafed',\n",
       " 'residuals',\n",
       " 'du',\n",
       " 'ncandidacy',\n",
       " 'equine',\n",
       " 'diffcult',\n",
       " 'ntobb',\n",
       " 'nsork',\n",
       " 'slhazy',\n",
       " 'hyperdnamic',\n",
       " 'morphologies',\n",
       " 'nslept',\n",
       " 'nvano',\n",
       " 'nleni',\n",
       " 'fortaz',\n",
       " 'keratopathy',\n",
       " 'corgard',\n",
       " 'neosinophilia',\n",
       " 'tied',\n",
       " 'heliax',\n",
       " 'hypercaoguable',\n",
       " 'nlen',\n",
       " 'nenroll',\n",
       " 'perifissural',\n",
       " 'ninvestigate',\n",
       " 'entamoeba',\n",
       " 'catheterizes',\n",
       " 'category',\n",
       " 'nylon',\n",
       " 'rrecent',\n",
       " 'melitu',\n",
       " 'signifant',\n",
       " 'nentirely',\n",
       " 'ndribbl',\n",
       " 'nceftazidine',\n",
       " 'functioningn',\n",
       " 'pasado',\n",
       " 'portacath',\n",
       " 'periumbilically',\n",
       " 'nsterotatic',\n",
       " 'nyha',\n",
       " 'lcta',\n",
       " 'acclerat',\n",
       " 'midbody',\n",
       " 'cornea',\n",
       " 'nhomemaker',\n",
       " 'agencie',\n",
       " 'invaive',\n",
       " 'ncurrrent',\n",
       " 'donnatal',\n",
       " 'hydrosalpinx',\n",
       " 'ntrash',\n",
       " 'conceivable',\n",
       " 'reshoot',\n",
       " 'npubi',\n",
       " 'religion',\n",
       " 'esomeprazole',\n",
       " 'supranormal',\n",
       " 'amazing',\n",
       " 'nunchangedwith',\n",
       " 'electrocardiograms',\n",
       " 'palvix',\n",
       " 'angiom',\n",
       " 'concur',\n",
       " 'nhemetology',\n",
       " 'zocar',\n",
       " 'dph',\n",
       " 'ntachybrady',\n",
       " 'cannualt',\n",
       " 'fwb',\n",
       " 'nsentinel',\n",
       " 'ganglial',\n",
       " 'ndeliriou',\n",
       " 'pyruvate',\n",
       " 'dhft',\n",
       " 'methodone',\n",
       " 'nclarinex',\n",
       " 'upsiz',\n",
       " 'npharygeal',\n",
       " 'angiogaphy',\n",
       " 'preferrence',\n",
       " 'tgrip',\n",
       " 'seoncadry',\n",
       " 'qweekly',\n",
       " 'petroleum',\n",
       " 'evalaution',\n",
       " 'nsorbitol',\n",
       " 'berenstein',\n",
       " 'nosteopetrosis',\n",
       " 'chvostek',\n",
       " 'indurated',\n",
       " 'npulmonolgist',\n",
       " 'nstippl',\n",
       " 'ndevelp',\n",
       " 'isoniazid',\n",
       " 'booking',\n",
       " 'npsychiarty',\n",
       " 'dstopp',\n",
       " 'nerod',\n",
       " 'ankylosi',\n",
       " 'reanastamosi',\n",
       " 'nmacrosteatosi',\n",
       " 'nax',\n",
       " 'stabhle',\n",
       " 'noveruse',\n",
       " 'riverwalk',\n",
       " 'sarafem',\n",
       " 'gangia',\n",
       " 'idio',\n",
       " 'resistence',\n",
       " 'anonymous',\n",
       " 'beers',\n",
       " 'nvegitation',\n",
       " 'definate',\n",
       " 'agencies',\n",
       " 'nick',\n",
       " 'elemental',\n",
       " 'ages',\n",
       " 'furosamide',\n",
       " 'snap',\n",
       " 'dps',\n",
       " 'nguaifenisin',\n",
       " 'xerofoam',\n",
       " 'amongst',\n",
       " 'nmonocryl',\n",
       " 'npreventative',\n",
       " 'nencephalopathie',\n",
       " 'nprovocation',\n",
       " 'nchoos',\n",
       " 'advsi',\n",
       " 'talonavicular',\n",
       " 'nhyperchloremic',\n",
       " 'budjet',\n",
       " 'intervascular',\n",
       " 'qntbiotic',\n",
       " 'smal',\n",
       " 'neigbor',\n",
       " 'preferred',\n",
       " 'caco',\n",
       " 'dyasrthia',\n",
       " 'nargon',\n",
       " 'differentiated',\n",
       " 'nslower',\n",
       " 'podrome',\n",
       " 'ntheatre',\n",
       " 'anesthia',\n",
       " 'ncarcinamotosi',\n",
       " 'tinpatient',\n",
       " 'sensaory',\n",
       " 'parnechymal',\n",
       " 'vacuolat',\n",
       " 'npneumo',\n",
       " 'peerla',\n",
       " 'atraumatice',\n",
       " 'cleard',\n",
       " 'woozy',\n",
       " 'nparaumbilical',\n",
       " 'tnot',\n",
       " 'erh',\n",
       " 'lexa',\n",
       " 'onychocryptosis',\n",
       " 'nhypertropy',\n",
       " 'distl',\n",
       " 'nmoniter',\n",
       " 'nuncu',\n",
       " 'nbreakpoints',\n",
       " 'remin',\n",
       " 'nmgmt',\n",
       " 'nleukostasi',\n",
       " 'paraverterbral',\n",
       " 'dicus',\n",
       " 'nupholsterer',\n",
       " 'namerican',\n",
       " 'nregime',\n",
       " 'crotch',\n",
       " 'acidez',\n",
       " 'kristalose',\n",
       " 'nbreathsound',\n",
       " 'supplies',\n",
       " 'metatarsu',\n",
       " 'pencil',\n",
       " 'naxilla',\n",
       " 'expanding',\n",
       " 'meatbolic',\n",
       " 'faithfully',\n",
       " 'nrvr',\n",
       " 'pneumatocele',\n",
       " 'abj',\n",
       " 'sterotactic',\n",
       " 'easlier',\n",
       " 'ntoco',\n",
       " 'thryoid',\n",
       " 'vasculitis',\n",
       " 'nmut',\n",
       " 'hyperchlesterolemia',\n",
       " 'reinsertion',\n",
       " 'nablify',\n",
       " 'scistosomiasi',\n",
       " 'occupant',\n",
       " 'meropenam',\n",
       " 'nglucometer',\n",
       " 'naura',\n",
       " 'forewarm',\n",
       " 'engine',\n",
       " 'nregiment',\n",
       " 'concenr',\n",
       " 'underwnet',\n",
       " 'octrotide',\n",
       " 'bilatereal',\n",
       " 'nglomerulosclerosis',\n",
       " 'monitorr',\n",
       " 'pencillin',\n",
       " 'nfxn',\n",
       " 'fremitu',\n",
       " 'nroxanol',\n",
       " 'nhemosiderosi',\n",
       " 'nallopruinol',\n",
       " 'npsychological',\n",
       " 'nzn',\n",
       " 'flaky',\n",
       " 'uhold',\n",
       " 'nextremety',\n",
       " 'interoperative',\n",
       " 'nindirect',\n",
       " 'nindbili',\n",
       " 'nangiomata',\n",
       " 'nmeniscal',\n",
       " 'retart',\n",
       " 'ncolonoscopie',\n",
       " 'prominant',\n",
       " 'insufficeny',\n",
       " 'esto',\n",
       " 'interdisciplinary',\n",
       " 'pairs',\n",
       " 'anerobic',\n",
       " 'mvrepair',\n",
       " 'menigioma',\n",
       " 'ncasperfungin',\n",
       " 'duret',\n",
       " 'nmultiloculat',\n",
       " 'mcgiv',\n",
       " 'nmicrobial',\n",
       " 'nantidepressant',\n",
       " 'nrcc',\n",
       " 'nbmt',\n",
       " 'appoitnement',\n",
       " 'metop',\n",
       " 'overeat',\n",
       " 'ntimeframe',\n",
       " 'primaquine',\n",
       " 'kidd',\n",
       " 'ehrlichieae',\n",
       " 'nhamburger',\n",
       " 'jersey',\n",
       " 'distractibility',\n",
       " 'downgrade',\n",
       " 'scoped',\n",
       " 'visualis',\n",
       " 'pressurew',\n",
       " 'satu',\n",
       " 'flushes',\n",
       " 'nthumbnail',\n",
       " 'lithotrpsie',\n",
       " 'bpbpr',\n",
       " 'novacaine',\n",
       " 'carvidopa',\n",
       " 'ncervicomedullary',\n",
       " 'nfractures',\n",
       " 'nadduction',\n",
       " 'iavp',\n",
       " 'nlovsatstain',\n",
       " 'assumed',\n",
       " 'nbandages',\n",
       " 'nacquisition',\n",
       " 'options',\n",
       " 'nfl',\n",
       " 'ocugh',\n",
       " 'nbell',\n",
       " 'discharges',\n",
       " 'breakfasst',\n",
       " 'kayexolate',\n",
       " 'coaptation',\n",
       " 'partical',\n",
       " 'perphenazine',\n",
       " 'tbr',\n",
       " 'ndysphasia',\n",
       " 'pavix',\n",
       " 'nventolin',\n",
       " 'shocks',\n",
       " 'cariomyopathy',\n",
       " 'dh',\n",
       " 'naortoenteric',\n",
       " 'rubbing',\n",
       " 'ndorsiflexor',\n",
       " 'contraindicated',\n",
       " 'nmyopathy',\n",
       " 'demarginalization',\n",
       " 'anagrelide',\n",
       " 'nerrors',\n",
       " 'olopatadine',\n",
       " 'phyiology',\n",
       " 'subarchnoid',\n",
       " 'nlessen',\n",
       " 'coaxial',\n",
       " 'ginger',\n",
       " 'extemitie',\n",
       " 'termninal',\n",
       " 'addend',\n",
       " 'ncrest',\n",
       " 'ngravid',\n",
       " 'enrolled',\n",
       " 'residence',\n",
       " 'neurooncologist',\n",
       " 'hyalin',\n",
       " 'binding',\n",
       " 'hepatocytes',\n",
       " 'acure',\n",
       " 'nmetatsarsal',\n",
       " 'nverifi',\n",
       " 'gte',\n",
       " 'sctarche',\n",
       " 'nrobust',\n",
       " 'ncatapre',\n",
       " 'craniotomies',\n",
       " 'nadanc',\n",
       " 'exotic',\n",
       " 'nfindid',\n",
       " 'nreanast',\n",
       " 'pnx',\n",
       " 'sandwiche',\n",
       " 'insidiou',\n",
       " 'azathiorpine',\n",
       " 'nhtxn',\n",
       " 'pathogenic',\n",
       " 'nleukoencephalopathy',\n",
       " 'striated',\n",
       " 'npreponderance',\n",
       " 'mobiltiy',\n",
       " 'attendings',\n",
       " 'maxide',\n",
       " 'nvegetative',\n",
       " 'tendinitis',\n",
       " 'leukocytoclastic',\n",
       " 'florastore',\n",
       " 'woundvac',\n",
       " 'anneurysm',\n",
       " 'debalk',\n",
       " 'stenet',\n",
       " 'folds',\n",
       " 'cdc',\n",
       " 'nconfident',\n",
       " 'pharyngiti',\n",
       " 'hypoperfus',\n",
       " 'conjugat',\n",
       " 'flows',\n",
       " 'meninge',\n",
       " 'nhepatopathy',\n",
       " 'intraventicular',\n",
       " 'enxyme',\n",
       " 'nreminiscent',\n",
       " 'vanv',\n",
       " 'nsnor',\n",
       " 'obta',\n",
       " 'notd',\n",
       " 'spare',\n",
       " 'autodirues',\n",
       " 'fusions',\n",
       " 'bosniak',\n",
       " 'nneomycin',\n",
       " 'pancreaiti',\n",
       " 'tquiet',\n",
       " 'arttery',\n",
       " 'nfremitu',\n",
       " 'nspell',\n",
       " 'paroxitene',\n",
       " 'nvault',\n",
       " 'elastase',\n",
       " 'ncyclophosphamide',\n",
       " 'sorethroat',\n",
       " 'nalphagin',\n",
       " 'midlung',\n",
       " 'resuming',\n",
       " 'transbroncheal',\n",
       " 'noccational',\n",
       " 'embolized',\n",
       " 'perisurgical',\n",
       " 'nlaparoscopy',\n",
       " 'coolness',\n",
       " 'ninstance',\n",
       " 'nwallet',\n",
       " 'diphragmatic',\n",
       " 'ambulated',\n",
       " 'nnorpace',\n",
       " 'nhypercapneic',\n",
       " 'nbovine',\n",
       " 'cefzil',\n",
       " 'flexed',\n",
       " 'atrail',\n",
       " 'temporoparieto',\n",
       " 'nsulfamethoxazole',\n",
       " 'restraine',\n",
       " 'tfick',\n",
       " 'epilepitform',\n",
       " 'gingko',\n",
       " 'prrl',\n",
       " 'nrecogniz',\n",
       " 'xweek',\n",
       " 'nadjunctive',\n",
       " 'profusion',\n",
       " 'possilbe',\n",
       " 'snore',\n",
       " 'extraventricular',\n",
       " 'whereupon',\n",
       " 'nestimate',\n",
       " 'fumigatus',\n",
       " 'originates',\n",
       " 'nshorten',\n",
       " 'nnonexertional',\n",
       " 'dyskinesia',\n",
       " 'nrasagaline',\n",
       " 'ncardiologust',\n",
       " 'substitution',\n",
       " 'compaint',\n",
       " 'tgc',\n",
       " 'gible',\n",
       " 'ntoal',\n",
       " 'anticoagulatn',\n",
       " 'pseudocapsule',\n",
       " 'nsse',\n",
       " 'acidic',\n",
       " 'patriot',\n",
       " 'ntherapuetic',\n",
       " 'leaking',\n",
       " 'abullic',\n",
       " 'miotic',\n",
       " 'nmulitple',\n",
       " 'lymphandopathy',\n",
       " 'ncauduet',\n",
       " 'nnafld',\n",
       " 'disruptive',\n",
       " 'nsib',\n",
       " 'anagiography',\n",
       " 'nasoenteric',\n",
       " 'radiocarpal',\n",
       " 'cardiolo',\n",
       " 'ventillation',\n",
       " 'interstial',\n",
       " 'ribaviron',\n",
       " 'nquinapril',\n",
       " 'ndysconjugate',\n",
       " 'attractive',\n",
       " 'nmohter',\n",
       " 'casofungin',\n",
       " 'nacetabula',\n",
       " 'streaky',\n",
       " 'doxyclycline',\n",
       " 'shakes',\n",
       " 'npresumtive',\n",
       " 'nusing',\n",
       " 'hemangiomatosi',\n",
       " 'macrophag',\n",
       " 'hospitilization',\n",
       " 'leptomeningiti',\n",
       " 'ospital',\n",
       " 'jiroveci',\n",
       " 'episoe',\n",
       " 'esophagectomy',\n",
       " 'identif',\n",
       " 'nsacrospinou',\n",
       " 'minitran',\n",
       " 'blushe',\n",
       " 'cytopenias',\n",
       " 'testicles',\n",
       " 'nmethylene',\n",
       " 'sundowning',\n",
       " 'decubitus',\n",
       " 'stc',\n",
       " 'nheterogenic',\n",
       " 'vagotomy',\n",
       " 'silicon',\n",
       " 'ndecubiti',\n",
       " 'nvisib',\n",
       " 'ngyburide',\n",
       " 'nsymptoms',\n",
       " 'hematobillia',\n",
       " 'ncgy',\n",
       " 'tulip',\n",
       " 'thrombogenic',\n",
       " 'unresponsibe',\n",
       " 'ausculation',\n",
       " 'rosc',\n",
       " 'hemotology',\n",
       " 'npresuure',\n",
       " 'noophorectomy',\n",
       " 'nicoderm',\n",
       " 'ndysplastic',\n",
       " 'gastrointesinal',\n",
       " 'dificult',\n",
       " 'bicapp',\n",
       " 'transeferd',\n",
       " 'tightly',\n",
       " 'nzesteretic',\n",
       " 'transbronchial',\n",
       " 'intr',\n",
       " 'npinch',\n",
       " 'nsuprapancreatic',\n",
       " 'vasculitide',\n",
       " 'nleiomyoma',\n",
       " 'hypogastrum',\n",
       " 'nmetaphysi',\n",
       " 'indurat',\n",
       " 'confinement',\n",
       " 'nwwp',\n",
       " 'nperle',\n",
       " 'attentuation',\n",
       " 'nchantil',\n",
       " 'hammock',\n",
       " 'poc',\n",
       " 'thrombosu',\n",
       " 'schizaffcetive',\n",
       " 'pseudoaa',\n",
       " 'akpop',\n",
       " 'tenacious',\n",
       " 'nstrictur',\n",
       " 'antiocagulation',\n",
       " 'nsobriety',\n",
       " 'spoon',\n",
       " 'alveoli',\n",
       " 'gsw',\n",
       " 'sido',\n",
       " 'choledocholiathiasis',\n",
       " 'nbreakup',\n",
       " 'tion',\n",
       " 'rape',\n",
       " 'surestep',\n",
       " 'ncarri',\n",
       " 'tih',\n",
       " 'exposing',\n",
       " 'calluse',\n",
       " 'interactine',\n",
       " 'ngranulocytic',\n",
       " 'avonex',\n",
       " 'keen',\n",
       " 'nroofer',\n",
       " 'npericolic',\n",
       " 'survery',\n",
       " 'dissapear',\n",
       " 'duty',\n",
       " 'nversion',\n",
       " 'noccassional',\n",
       " 'rmg',\n",
       " 'pneumovac',\n",
       " 'atacand',\n",
       " 'erratic',\n",
       " 'nephrostomie',\n",
       " 'thymus',\n",
       " 'crohns',\n",
       " 'nbronchioliti',\n",
       " 'chm',\n",
       " 'profilnine',\n",
       " 'nther',\n",
       " 'diruesis',\n",
       " 'nervosa',\n",
       " 'amterior',\n",
       " 'ntapazole',\n",
       " 'yor',\n",
       " 'colonsocopy',\n",
       " 'busulfan',\n",
       " 'ndialysate',\n",
       " 'supervene',\n",
       " 'aerosolized',\n",
       " 'arachnoiditi',\n",
       " 'lense',\n",
       " 'pindolol',\n",
       " 'nredundancy',\n",
       " 'weds',\n",
       " 'eustachian',\n",
       " 'eucerin',\n",
       " 'improvemetn',\n",
       " 'ronchorou',\n",
       " 'leuko',\n",
       " 'extertion',\n",
       " 'npateint',\n",
       " 'ndetoxification',\n",
       " 'cranially',\n",
       " 'tranfusion',\n",
       " 'ndevelopp',\n",
       " 'eoemi',\n",
       " 'subaxial',\n",
       " 'nmatche',\n",
       " 'cuasing',\n",
       " 'absorbtion',\n",
       " 'samarium',\n",
       " 'nibs',\n",
       " 'gentlamen',\n",
       " 'stared',\n",
       " 'nferrex',\n",
       " 'nrecentcypher',\n",
       " 'cou',\n",
       " 'sperm',\n",
       " 'nmalignancie',\n",
       " 'nmeropenum',\n",
       " 'nantecubital',\n",
       " 'feldburg',\n",
       " 'percentile',\n",
       " 'nifn',\n",
       " 'nsinusitis',\n",
       " 'sevice',\n",
       " 'nvertebrobasilar',\n",
       " 'experiences',\n",
       " 'confabulation',\n",
       " 'protuberance',\n",
       " 'ls',\n",
       " 'nneulasta',\n",
       " 'thush',\n",
       " 'stunned',\n",
       " 'glascow',\n",
       " 'demosntrat',\n",
       " 'npeerl',\n",
       " 'thyroglobulin',\n",
       " 'ngroom',\n",
       " 'gprs',\n",
       " 'actres',\n",
       " 'renall',\n",
       " 'hematemsi',\n",
       " 'pavilion',\n",
       " 'peaking',\n",
       " 'osmostat',\n",
       " 'armpit',\n",
       " 'mandatory',\n",
       " 'words',\n",
       " 'sulcralfate',\n",
       " 'artee',\n",
       " 'lligation',\n",
       " 'nalufosin',\n",
       " 'nfeversv',\n",
       " 'perforate',\n",
       " 'branched',\n",
       " 'nnonemergent',\n",
       " 'excell',\n",
       " 'utt',\n",
       " 'downgrad',\n",
       " 'nplacment',\n",
       " 'preciptins',\n",
       " 'nmath',\n",
       " 'reticulation',\n",
       " 'bingo',\n",
       " 'tol',\n",
       " 'nperibronchovascular',\n",
       " 'adluscent',\n",
       " 'swisha',\n",
       " 'nperclose',\n",
       " 'nonreative',\n",
       " 'npseudomonal',\n",
       " 'bv',\n",
       " 'nthiazides',\n",
       " 'ndishwasher',\n",
       " 'seratonergic',\n",
       " 'posi',\n",
       " 'npainles',\n",
       " 'rechecked',\n",
       " 'suppressive',\n",
       " 'lisionpril',\n",
       " 'wnc',\n",
       " 'intially',\n",
       " 'nfight',\n",
       " 'regenerat',\n",
       " 'nzenker',\n",
       " 'nglomeruli',\n",
       " 'trbc',\n",
       " 'nmaintainance',\n",
       " 'nrectu',\n",
       " 'corticated',\n",
       " 'nspondylophyte',\n",
       " 'tfluid',\n",
       " 'nrelaxant',\n",
       " 'prh',\n",
       " 'lightened',\n",
       " 'ndilaysi',\n",
       " 'marijuan',\n",
       " 'tryptase',\n",
       " 'nhemodynamics',\n",
       " 'procaine',\n",
       " 'vascularized',\n",
       " 'cellept',\n",
       " 'augmantin',\n",
       " 'ophth',\n",
       " 'janumet',\n",
       " 'rechallenge',\n",
       " 'nsepta',\n",
       " 'hairpin',\n",
       " 'laotian',\n",
       " 'chronotropy',\n",
       " 'tths',\n",
       " 'lingual',\n",
       " 'ndroplet',\n",
       " 'nnotdone',\n",
       " 'noncalcified',\n",
       " 'lscta',\n",
       " 'aps',\n",
       " 'superintedant',\n",
       " 'undercooked',\n",
       " 'dily',\n",
       " 'nnod',\n",
       " 'abstience',\n",
       " 'nseventy',\n",
       " 'regulate',\n",
       " 'lobotomy',\n",
       " 'ndt',\n",
       " 'comprised',\n",
       " 'amino',\n",
       " 'ntransfalcine',\n",
       " 'democracy',\n",
       " 'ivac',\n",
       " 'ndizzynes',\n",
       " 'decleration',\n",
       " 'nmol',\n",
       " 'ntoes',\n",
       " 'temporize',\n",
       " 'sacroliiti',\n",
       " 'noverhydration',\n",
       " 'erythropoiesis',\n",
       " 'rcm',\n",
       " 'collagen',\n",
       " 'gii',\n",
       " 'nhypertriglyceridemia',\n",
       " 'nnitrostat',\n",
       " 'psap',\n",
       " 'deeply',\n",
       " 'appos',\n",
       " 'abdomninal',\n",
       " 'nbracelet',\n",
       " 'hyperproteinemia',\n",
       " 'nbank',\n",
       " 'nunpleasant',\n",
       " 'errla',\n",
       " 'hyperlipdemia',\n",
       " 'nfought',\n",
       " 'pincare',\n",
       " 'subsalicy',\n",
       " 'admssion',\n",
       " 'fk',\n",
       " 'classifier',\n",
       " 'petoxiphylline',\n",
       " 'eneter',\n",
       " 'nrsca',\n",
       " 'upslop',\n",
       " 'intermuscular',\n",
       " 'nindpendent',\n",
       " 'seconary',\n",
       " 'ndeform',\n",
       " 'reveiw',\n",
       " 'filgastrim',\n",
       " 'nephrotoxicity',\n",
       " 'acidose',\n",
       " 'nhaptoglob',\n",
       " 'mpcwp',\n",
       " 'ntriglycer',\n",
       " 'oreient',\n",
       " 'talar',\n",
       " 'tiban',\n",
       " 'endemic',\n",
       " 'nhamartoma',\n",
       " 'remedicine',\n",
       " 'ndiscretion',\n",
       " 'nifediac',\n",
       " 'exerted',\n",
       " 'intermedia',\n",
       " 'tcd',\n",
       " 'fdi',\n",
       " 'nauseau',\n",
       " 'understands',\n",
       " 'diliatem',\n",
       " 'fuel',\n",
       " 'negx',\n",
       " 'nshigella',\n",
       " 'tnad',\n",
       " 'cleaning',\n",
       " 'customer',\n",
       " 'ncrisp',\n",
       " 'nsixth',\n",
       " 'ncholecystecomy',\n",
       " 'npeforat',\n",
       " 'partime',\n",
       " 'gridiron',\n",
       " 'gastointestinal',\n",
       " 'nregret',\n",
       " 'opthamology',\n",
       " 'midol',\n",
       " 'cramping',\n",
       " 'subhepatic',\n",
       " 'phencyclidine',\n",
       " 'perpheral',\n",
       " 'referal',\n",
       " 'vaccinated',\n",
       " 'ngrunt',\n",
       " 'asvd',\n",
       " 'outpouching',\n",
       " 'intracondylar',\n",
       " 'untoward',\n",
       " 'nsmallest',\n",
       " 'malleate',\n",
       " 'negativ',\n",
       " 'nanticoagulaiton',\n",
       " 'parecentesi',\n",
       " 'npreferr',\n",
       " 'njejunostomy',\n",
       " 'spam',\n",
       " 'tempature',\n",
       " 'bivalirudin',\n",
       " 'drianage',\n",
       " 'npushbutton',\n",
       " 'ncefotetan',\n",
       " 'nsubcorneal',\n",
       " 'scx',\n",
       " 'parece',\n",
       " 'nbillroth',\n",
       " 'nabrasive',\n",
       " 'labe',\n",
       " 'compartement',\n",
       " 'nsphincterotomize',\n",
       " 'corporate',\n",
       " 'professional',\n",
       " 'nbecoming',\n",
       " 'nangigoraphical',\n",
       " 'usu',\n",
       " 'hypercoag',\n",
       " 'hrct',\n",
       " 'nbulge',\n",
       " 'diastoic',\n",
       " 'tcp',\n",
       " 'deambulate',\n",
       " 'eunatremic',\n",
       " 'golytely',\n",
       " 'heimlich',\n",
       " 'sohendra',\n",
       " 'nthreat',\n",
       " 'pradaxa',\n",
       " 'auricular',\n",
       " 'nwan',\n",
       " 'nprofunda',\n",
       " 'roast',\n",
       " 'fluorescence',\n",
       " 'zoysn',\n",
       " 'admin',\n",
       " 'chilled',\n",
       " 'ndevot',\n",
       " 'oligouria',\n",
       " 'yf',\n",
       " 'nfibroproliferative',\n",
       " 'nassocaiat',\n",
       " 'worried',\n",
       " 'folfiri',\n",
       " 'hind',\n",
       " 'regularity',\n",
       " 'nanteriorly',\n",
       " 'elongated',\n",
       " 'imipenan',\n",
       " 'phsyician',\n",
       " 'nthrombophlebitis',\n",
       " 'nhypoechogenicity',\n",
       " 'therpay',\n",
       " 'nhuge',\n",
       " 'palms',\n",
       " 'multile',\n",
       " 'nheadahce',\n",
       " 'nqfri',\n",
       " 'bood',\n",
       " 'interfere',\n",
       " 'nautoregulation',\n",
       " 'ndebri',\n",
       " 'headedness',\n",
       " 'colapse',\n",
       " 'nvodka',\n",
       " 'topogram',\n",
       " 'inflatable',\n",
       " 'vsurg',\n",
       " 'hypercapneic',\n",
       " 'patientwould',\n",
       " 'nmenopausal',\n",
       " 'nrad',\n",
       " 'npasse',\n",
       " 'qmowefr',\n",
       " 'rehabe',\n",
       " 'supports',\n",
       " 'ndisk',\n",
       " 'cochlear',\n",
       " 'neastern',\n",
       " 'copay',\n",
       " 'treate',\n",
       " 'nbacteremic',\n",
       " 'tnarcotic',\n",
       " 'triamtrine',\n",
       " 'aneurysem',\n",
       " 'gammma',\n",
       " 'transmittyed',\n",
       " 'fork',\n",
       " 'nsalpingectomy',\n",
       " 'nmsm',\n",
       " 'durign',\n",
       " 'ndiscernable',\n",
       " 'nanticoaqgulationi',\n",
       " 'hypotenstion',\n",
       " 'oculi',\n",
       " 'nsimplify',\n",
       " 'unlcear',\n",
       " ...]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notes\n",
    "# sum([np.exp(1)** x for x in nb_classifier.coef_[0]]) # The probability of all the words equals one\n",
    "# # Taken from here: * https://stackoverflow.com/questions/61586946/how-to-calculate-feature-log-prob-in-the-naive-bayes-multinomialnb\n",
    "\n",
    "# ------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_feature_rank(tfidf_vectorizer, y_no, nb_classifier):\n",
    "    \n",
    "    # Get the feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # Zip together the first CPT weights with feature names\n",
    "    feat_with_weights =  sorted(zip(nb_classifier.coef_[y_no], feature_names))\n",
    "    \n",
    "    # Print words most responsible for the prediction\n",
    "#     print('Top 100 \\n\\n\\n\\n')\n",
    "#     top_100_ls = []\n",
    "    for i in range(100):\n",
    "        x = feat_with_weights[-i-1]\n",
    "#         top_100_ls.append(x[1])\n",
    "#         print(nb_classifier.classes_[y_no], i, round((np.exp(1) ** x[0]),4), x[1])\n",
    "\n",
    "#     print('\\n\\n\\n\\n Bottom 100 \\n\\n\\n\\n')\n",
    "    for i in range(100):\n",
    "        x = feat_with_weights[i]\n",
    "#         print(nb_classifier.classes_[y_no], i, round((np.exp(1) ** x[0]),4), x[1])\n",
    "    \n",
    "#     min_weight = min([i[0] for i in feat_with_weights])\n",
    "    \n",
    "    x = [i[0] for i in feat_with_weights]\n",
    "    \n",
    "    median_pred = np.median(x)\n",
    "          \n",
    "    return [i[1] for i in feat_with_weights if i[0] <= median_pred] # Minimum weight words\n",
    "#     return top_100_ls\n",
    "\n",
    "# Find the least predictive words\n",
    "def least_pred_words(nb_classifier, tfidf_vectorizer):\n",
    "    low_wt_stop_ls = []\n",
    "\n",
    "    for i in range(len(nb_classifier.classes_)):\n",
    "        low_wt_stop_ls += get_feature_rank(tfidf_vectorizer, i, nb_classifier)\n",
    "\n",
    "    low_wt_stop_ls = list(set(low_wt_stop_ls))\n",
    "    return low_wt_stop_ls\n",
    "    \n",
    "low_wt_stop_ls = least_pred_words(nb_classifier, tfidf_vectorizer)\n",
    "\n",
    "# Find top 100 words - doesn't seem to improve the model\n",
    "def highest_pred_words(nb_classifier, tfidf_vectorizer):\n",
    "    top_100_ls = []\n",
    "    for i in range(len(nb_classifier.classes_)):\n",
    "        top_100_ls += get_feature_rank(tfidf_vectorizer, i, nb_classifier)\n",
    "\n",
    "    top_100_ls = list(set(top_100_ls))\n",
    "    return top_100_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-summit",
   "metadata": {},
   "source": [
    "# Update stop words and tokenize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "intense-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words += low_wt_stop_ls\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=my_stop_words, min_df = 3, max_df = .7, sublinear_tf=True)\n",
    "\n",
    "# Transform the training data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-surface",
   "metadata": {},
   "source": [
    "# Create Vocab with top words and tokenize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It reduced test accuracy back to 43% and training went from 50% to 44%\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=top_100_ls, stop_words=my_stop_words, min_df = 3, max_df = .7, sublinear_tf=True)\n",
    "\n",
    "# Transform the training data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-remove",
   "metadata": {},
   "source": [
    "# Run Naive Bayes again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "painted-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Naive Bayes model -----\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB(alpha=.7)\n",
    "\n",
    "# Fit and check accuracy\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "pred = nb_classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-kenya",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "optimum-turkish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-66833f5c5a4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclass_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2033\u001b[0m         \u001b[1;31m# compute averages with specified averaging method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2034\u001b[1;33m         avg_p, avg_r, avg_f1, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[0;32m   2035\u001b[0m             \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m             \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1464\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m     \u001b[0msamplewise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'samples'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1466\u001b[1;33m     MCM = multilabel_confusion_matrix(y_true, y_pred,\n\u001b[0m\u001b[0;32m   1467\u001b[0m                                       \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m                                       labels=labels, samplewise=samplewise)\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[1;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpresent_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# Check that we don't mix label format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mys_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mys_types\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mys_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# Check that we don't mix label format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mys_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mys_types\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mys_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'continuous'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'multiclass'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msuffix\u001b[0m  \u001b[1;31m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[0mar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create classification report taken from here: https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Test')\n",
    "class_labels = nb_classifier.classes_\n",
    "print(classification_report(y_test, pred,target_names=class_labels))\n",
    "\n",
    "print('Training')\n",
    "pred_x = nb_classifier.predict(tfidf_train)\n",
    "print(classification_report(y_train, pred_x,target_names=class_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "italian-serum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3441613031558106"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "# \"\"\"\n",
    "# V1 NLP Model Accuracy: 0.117\n",
    "# Wow, I've got a long way to go to improve accuracy\n",
    "# V2 NLP Model Accuracy: 0.14\n",
    "# V3 NLP Model Accuracy: .40\n",
    "# \"\"\"\n",
    "\n",
    "# Confusion matrix \n",
    "# confusion_mtrx = metrics.confusion_matrix(y_test.astype(str), pred) # 1380, 1380\n",
    "# confusion_mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "quantitative-leadership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3560782168740599"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistical Model accuracy\n",
    "metrics.accuracy_score(y_test, logist_pred)\n",
    "# .39\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-wages",
   "metadata": {},
   "source": [
    "# Vectorize Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "objective-registrar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 2)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab = ['love', 'happy', 'run']\n",
    "count_vectorizer = CountVectorizer(vocabulary = vocab)\n",
    "x = count_vectorizer.fit_transform(['happy', 'run', 'run', 'run'])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-message",
   "metadata": {},
   "source": [
    "# Splitting out a list in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "human-redhead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The other day</td>\n",
       "      <td>I saw a bear</td>\n",
       "      <td>A great big bear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0              1                  2\n",
       "0  The other day   I saw a bear   A great big bear"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.DataFrame({'column 1':['The other day, I saw a bear, A great big bear',2,3], 'predictive column':[1,2,3]} )\n",
    "test['column 1'].str.split(',', expand=True).drop(labels=[1,2], axis=0)\n",
    "# pd.concat([test['column 1'].str.split(',', expand=True), test['predictive column']], axis=1).melt(id_vars='predictive column').drop('variable', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "noble-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.DataFrame.drop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
