{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "built-romance",
   "metadata": {},
   "source": [
    "# Changes from v2\n",
    "\n",
    "* Subset to just the 4 CPT codes that the model was predicting:   99291, 99232, 94003, 99233 \n",
    "* Ideas for next improvement:\n",
    "    * Check for class imbalance in v2\n",
    "    * Check for class imbalance in v3\n",
    "    * Text cleaning\n",
    "    * Try count vectors\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-alabama",
   "metadata": {},
   "source": [
    "# Import the MIMIC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dominant-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (4,5,7,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "dataset_dictionary = {}\n",
    "\n",
    "for file_path in glob.glob('.\\\\Data\\\\MIMIC Files\\*'):\n",
    "    file_name = file_path.split('\\\\')[3].split('.')[0]\n",
    "    with gzip.open(file_path, mode='r') as file:\n",
    "        dataset_dictionary[file_name] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-desert",
   "metadata": {},
   "source": [
    "# Join the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "grateful-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset to join together -----\n",
    "\n",
    "# Create note_events table -----\n",
    "\n",
    "# Combine text for each subject and encounter\n",
    "note_events_base = dataset_dictionary['NOTEEVENTS'][dataset_dictionary['NOTEEVENTS'].loc[:,'CATEGORY'] == 'Discharge summary']\n",
    "note_events = note_events_base.groupby(['SUBJECT_ID', 'HADM_ID'], as_index=False)['TEXT'].agg(sum)\n",
    "\n",
    "# Create CPT table -----\n",
    "\n",
    "cpt_events_base = dataset_dictionary['CPTEVENTS'].loc[:, ['SUBJECT_ID','HADM_ID', 'CPT_CD']]\n",
    "cpt_events = cpt_events_base.drop_duplicates()\n",
    "\n",
    "# Join the datasets -----\n",
    "\n",
    "note_cpt = note_events.merge(cpt_events, on = ['SUBJECT_ID','HADM_ID'])\n",
    "# print(note_cpt.shape, note_events.shape, cpt_events.shape) # (223,150, 4) (52,726, 3) (227,510, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-council",
   "metadata": {},
   "source": [
    "# Filter the data to CPT with over 1000 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "resident-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find CPT codes occurring over 1000 times and put into a list\n",
    "cpt_1000 = note_cpt['CPT_CD'].astype(str).value_counts(ascending=False) >= 1000\n",
    "cpt_1000_ls = list(cpt_1000[cpt_1000].index)\n",
    "\n",
    "note_cpt_1000 = note_cpt[note_cpt['CPT_CD'].astype(str).isin(cpt_1000_ls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "liquid-tract",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>CPT_CD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>145834.0</td>\n",
       "      <td>Admission Date:  [**2101-10-20**]     Discharg...</td>\n",
       "      <td>94002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>145834.0</td>\n",
       "      <td>Admission Date:  [**2101-10-20**]     Discharg...</td>\n",
       "      <td>94003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID   HADM_ID                                               TEXT  \\\n",
       "0           3  145834.0  Admission Date:  [**2101-10-20**]     Discharg...   \n",
       "1           3  145834.0  Admission Date:  [**2101-10-20**]     Discharg...   \n",
       "\n",
       "  CPT_CD  \n",
       "0  94002  \n",
       "1  94003  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_cpt_1000[note_cpt_1000['HADM_ID'] == 145834]\n",
    "# .loc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-dealing",
   "metadata": {},
   "source": [
    "# Filter the data to just 4 CPT codes: 99291, 99232, 94003, 99233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "pursuant-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_cpt_4 = note_cpt[note_cpt['CPT_CD'].astype(str).isin(['99291', '99232', '94003'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-devil",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "extensive-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages -----\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split the data -----\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(note_cpt_4['TEXT'], note_cpt_4['CPT_CD'].astype(str), test_size = .33, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-comment",
   "metadata": {},
   "source": [
    "# Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "determined-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data -----\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df = .7)\n",
    "\n",
    "# Transform the training data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# print(tfidf_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-grenada",
   "metadata": {},
   "source": [
    "# Run Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cultural-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Naive Bayes model -----\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit and check accuracy\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "pred = nb_classifier.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-kenya",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "optimum-turkish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       94003       0.76      0.03      0.06      5456\n",
      "       99232       0.33      0.15      0.21      7937\n",
      "       99291       0.41      0.86      0.56      8546\n",
      "\n",
      "    accuracy                           0.40     21939\n",
      "   macro avg       0.50      0.35      0.27     21939\n",
      "weighted avg       0.47      0.40      0.31     21939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create classification report taken from here: https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "print(classification_report(y_test, pred,target_names=class_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-ultimate",
   "metadata": {},
   "source": [
    "# Looking at Feature Names and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "decent-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = nb_classifier.classes_\n",
    "# len(class_labels) # 24\n",
    "\n",
    "# len(nb_classifier.coef_) # 24\n",
    "# (nb_classifier.coef_[0] ==  nb_classifier.coef_[1]) # They are unique to each CPT code, but are for the same features\n",
    "\n",
    "# len(feature_names) # 207,838\n",
    "\n",
    "# Zip together the first CPT weights with feature names\n",
    "# feat_with_weights =  sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# # Class label with weights\n",
    "# print(class_labels[0])\n",
    "\n",
    "# for i in range(100):\n",
    "#     print(i, feat_with_weights[-i])\n",
    "\n",
    "# # Print dataframe # ran into memory error\n",
    "# # (tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "# # tfidf_df.head()\n",
    "# import numpy as np\n",
    "\n",
    "# np.exp(1)**-8.5\n",
    "# # Taken from here: * https://stackoverflow.com/questions/61586946/how-to-calculate-feature-log-prob-in-the-naive-bayes-multinomialnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "italian-serum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3991977756506678"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "# \"\"\"\n",
    "# V1 NLP Model Accuracy: 0.117\n",
    "# Wow, I've got a long way to go to improve accuracy\n",
    "# V2 NLP Model Accuracy: 0.14\n",
    "# V3 NLP Model Accuracy: .40\n",
    "# \"\"\"\n",
    "\n",
    "# Confusion matrix \n",
    "# confusion_mtrx = metrics.confusion_matrix(y_test.astype(str), pred) # 1380, 1380\n",
    "# confusion_mtrx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
