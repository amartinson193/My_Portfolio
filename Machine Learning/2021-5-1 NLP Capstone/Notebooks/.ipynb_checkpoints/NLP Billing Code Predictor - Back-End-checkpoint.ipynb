{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expired-sharing",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "answering-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "from sklearn.utils import resample\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-alabama",
   "metadata": {},
   "source": [
    "\n",
    "# Import the data from MIMIC-III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-consultation",
   "metadata": {},
   "source": [
    "Importing 8 tables from the MIMIC-III database, all of them are related to ICD and CPT codes and the related clinical notes. I store this information in a dictionary that I can pull from later on in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-character",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (4,5,7,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "dataset_dictionary = {}\n",
    "\n",
    "for file_path in glob.glob('..\\\\Data\\\\MIMIC Files\\*'):\n",
    "    file_name = file_path.split('\\\\')[3].split('.')[0]\n",
    "    with gzip.open(file_path, mode='r') as file:\n",
    "        dataset_dictionary[file_name] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dictionary.keys()\n",
    "# dataset_dictionary['D_ICD_DIAGNOSES']\n",
    "# dataset_dictionary['D_CPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dictionary['CPTEVENTS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-literature",
   "metadata": {},
   "source": [
    "# Assign Datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-cardiff",
   "metadata": {},
   "source": [
    "This section cleans up the code and fixes any datatype issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all the datasets exist in the dictionary \n",
    "print(dataset_dictionary.keys())\n",
    "\n",
    "# Check the datatypes and information for each table \n",
    "for i in dataset_dictionary.keys():\n",
    "    print(dataset_dictionary[i].info())\n",
    "\n",
    "# Correct any datatype issues #####\n",
    "\n",
    "# CPTEVENTS\n",
    "dataset_dictionary['CPTEVENTS'].loc[:,['SECTIONHEADER','CPT_CD']] = dataset_dictionary['CPTEVENTS'].loc[:,['SECTIONHEADER','CPT_CD']].astype(str)\n",
    "dataset_dictionary['CPTEVENTS']['CHARTDATE'] = dataset_dictionary['CPTEVENTS']['CHARTDATE'].to_datetime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-desert",
   "metadata": {},
   "source": [
    "# Join the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-sailing",
   "metadata": {},
   "source": [
    "The ICD and CPT codes needed to be joined to the clinical notes. Also, a little bit of data cleaning needed to be done. The clinical notes were limited to just the discharge summaries since that helps to capture an overview of the patient encounter. The original number of clinical notes was 2,083,180 and filtering the category to discharge summary reduced it to 223,571 notes. The description type is also filtered to just report. The joins were made on the subject ID and HADM ID (encounter ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tables(dataset_dictionary, category=['Discharge summary'], all_notes=False):\n",
    "\n",
    "    # Define tables\n",
    "    note_events_base = dataset_dictionary['NOTEEVENTS']\n",
    "    cpt_events_base = dataset_dictionary['CPTEVENTS']\n",
    "    icd_events_base = dataset_dictionary['DIAGNOSES_ICD']\n",
    "\n",
    "    # Combine text for each subject and encounter\n",
    "    if all_notes == False:\n",
    "        note_events_base = note_events_base[note_events_base.loc[:,'CATEGORY'].isin(category)]\n",
    "    \n",
    "    # Filter out the addendums and restrict notes only to reports\n",
    "    note_events_base = note_events_base[note_events_base['DESCRIPTION'] == 'Report']\n",
    "    \n",
    "    # Aggregate text by Subject and HADM ID's\n",
    "    note_events = note_events_base.groupby(['SUBJECT_ID', 'HADM_ID'], as_index=False)['TEXT'].agg(sum)\n",
    "    \n",
    "    # Create CPT table\n",
    "    cpt_events_base = cpt_events_base.loc[:, ['SUBJECT_ID','HADM_ID', 'CPT_CD', 'SECTIONHEADER', 'DESCRIPTION']]\n",
    "    cpt_events = cpt_events_base.drop_duplicates()\n",
    "    \n",
    "    # Create ICD table\n",
    "    icd_events_base = icd_events_base[icd_events_base['SEQ_NUM'] == 1]\n",
    "    icd_events_base = icd_events_base.loc[:, ['SUBJECT_ID','HADM_ID', 'ICD9_CODE']]\n",
    "    icd_events = icd_events_base.drop_duplicates()\n",
    "    \n",
    "    # Join the datasets\n",
    "    note_cpt = note_events.merge(cpt_events, on = ['SUBJECT_ID','HADM_ID'])\n",
    "    note_icd = note_events.merge(icd_events, on = ['SUBJECT_ID', 'HADM_ID'])\n",
    "    \n",
    "    # Replace any nulls with blanks\n",
    "    x = note_cpt[note_cpt['DESCRIPTION'].isnull()].copy()\n",
    "    x.loc[:,'DESCRIPTION'] = ''\n",
    "    y = note_cpt[note_cpt['DESCRIPTION'].notnull()].copy()\n",
    "    note_cpt = pd.concat([x,y])\n",
    "    \n",
    "    # Combine description and text columns\n",
    "    note_cpt['TEXT'] = note_cpt['TEXT'] + note_cpt['DESCRIPTION']\n",
    "    note_cpt = note_cpt.drop('DESCRIPTION', axis=1)\n",
    "    \n",
    "    return note_cpt, note_icd\n",
    "\n",
    "# Run the function\n",
    "note_cpt, note_icd = join_tables(dataset_dictionary)\n",
    "\n",
    "# Drop notes with the nan sectionheader\n",
    "drop_ls = note_cpt[note_cpt['SECTIONHEADER'] == 'nan']\n",
    "note_cpt = note_cpt.drop(drop_ls.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dictionary['PATIENTS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-dealing",
   "metadata": {},
   "source": [
    "# Filter the data - CPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-spiritual",
   "metadata": {},
   "source": [
    "A number of different functions were made which helped to filter the CPT codes. This was part of the preprocessing process and the steps that were made here improved accuracy in the model a lot. In the end, I partitioned the CPT codes by the CPT Category Code Level 1. There are 6 sections. I took the top 5 from each one. I also filtered to only codes that had 100 notes or more to help the machine learning model learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pursuant-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(combined_df, threshold):\n",
    "\n",
    "    # Print value counts original\n",
    "    print('Value Counts for the original data:\\n\\n', combined_df['CPT_CD'].value_counts().head(25))\n",
    "\n",
    "    # Filter based on count limit\n",
    "    df = combined_df['CPT_CD'].value_counts()\n",
    "    filtered_ls = list((df[df >= threshold]).index.values)\n",
    "    filtered_df = combined_df[combined_df['CPT_CD'].isin(filtered_ls)]\n",
    "    \n",
    "    # Print value counts filtered\n",
    "    print('Value Counts for the filtered data:\\n\\n', combined_df['CPT_CD'].value_counts().head(25))\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Find Counts of CPT Codes per Patient Encounter and filter df\n",
    "def cpt_count_filter(df, og_df, cpt_section_hadm_limit, cpt_hadm_limit):\n",
    "    \n",
    "    # Filter based on limit per SECTIONHEADER & merge\n",
    "    df1 = og_df.groupby(['HADM_ID', 'SECTIONHEADER'])['CPT_CD'].count()\n",
    "    filtered_encntrs = df1[df1 <= cpt_section_hadm_limit]\n",
    "    final_df = df.merge(filtered_encntrs, on=['HADM_ID', 'SECTIONHEADER'])\n",
    "    final_df.drop('CPT_CD_y', axis=1, inplace=True)\n",
    "    \n",
    "    # Filter dataset again based on total number of CPT codes per HADM\n",
    "    df2 = og_df.groupby(['HADM_ID'])['CPT_CD'].count()\n",
    "    filtered_encntrs = df2[df2 <= cpt_hadm_limit]\n",
    "    final_df = final_df.merge(filtered_encntrs, on=['HADM_ID'])\n",
    "    final_df.drop('CPT_CD', axis=1, inplace=True)\n",
    "    \n",
    "    # Rename columns\n",
    "    final_df.columns = ['SUBJECT_ID', 'HADM_ID', 'TEXT', 'CPT_CD', 'SECTIONHEADER']\n",
    "    \n",
    "    print('Value Counts for the filtered data:\\n\\n', final_df['CPT_CD'].value_counts().head(50))\n",
    "\n",
    "    return final_df\n",
    "    \n",
    "# Filter DataFrame to a set amount of CPT codes in each section header #####\n",
    "def cpt_per_section_filter(df, section_limit, sections=['Emerging technology'], all_sections=False):\n",
    "    \n",
    "    # Create list for dataframes\n",
    "    df_ls = []\n",
    "\n",
    "    # Group by and count the number of CPT codes\n",
    "    cts_by_cpt = df.groupby(['SECTIONHEADER', 'CPT_CD'])['CPT_CD'].count()\n",
    "    cts_by_cpt.index.names = ['SECTIONHEADER', 'CPT_CDS']\n",
    "    cts_by_cpt = cts_by_cpt.reset_index()\n",
    "    cts_by_cpt.columns = ['SECTIONHEADER', 'CPT_CD', 'COUNT']\n",
    "\n",
    "    # Sort values by section and CPT code count\n",
    "    cts_by_cpt_s = cts_by_cpt.sort_values(by=['SECTIONHEADER','COUNT'], ascending=False)\n",
    "\n",
    "    # Filter based on the limit of CPT codes wanted for each category\n",
    "    if all_sections == True:\n",
    "        sections = list(set(df['SECTIONHEADER']))\n",
    "        \n",
    "    for i in sections:\n",
    "        top_cts = cts_by_cpt_s[cts_by_cpt_s['SECTIONHEADER'] == i].iloc[:section_limit,:]\n",
    "\n",
    "        # Append to list\n",
    "        df_ls.append(top_cts)\n",
    "\n",
    "    # Combine DataFrames\n",
    "    df_combo = pd.concat(df_ls)\n",
    "\n",
    "    # Join back to source data\n",
    "    final_df = df.merge(df_combo, on=['SECTIONHEADER','CPT_CD'])\n",
    "    \n",
    "    print('\\nThe length of the initial dataset was {} and the new dataset is {}\\n\\n'.format(len(df), len(final_df)))\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def show_section_counts(df):\n",
    "    # Print count of CPT codes by section\n",
    "    cts_by_cpt = df.groupby(['SECTIONHEADER', 'CPT_CD'])['CPT_CD'].count()\n",
    "    cts_by_section = cts_by_cpt.groupby('SECTIONHEADER').count()\n",
    "    print('\\nHere are the counts by section:\\n\\n', cts_by_section)\n",
    "\n",
    "def show_cpt_counts_by_section(df):\n",
    "    cts_by_cpt = df.groupby(['SECTIONHEADER', 'CPT_CD'])['CPT_CD'].count()\n",
    "    print('\\nHere are the counts by CPT by section\\n\\n:')\n",
    "#     print(pd.DataFrame(cts_by_cpt.rename('Count')).reset_index())\n",
    "    for i, x in pd.DataFrame(cts_by_cpt.rename('Count')).reset_index().iterrows():\n",
    "        print(x['SECTIONHEADER'], x['CPT_CD'], x['Count'])\n",
    "        \n",
    "def filter_cpt_ct(df, section, ct):\n",
    "    df = df[df['SECTIONHEADER'] == section]\n",
    "    top_codes = list(df['CPT_CD'].value_counts().head(ct).index)\n",
    "    df = df[df.CPT_CD.isin(top_codes)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "encouraging-music",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Counts for the filtered data:\n",
      "\n",
      " 94003    12871\n",
      "94002     8206\n",
      "99291     4758\n",
      "99232     2385\n",
      "99233     1924\n",
      "36556     1788\n",
      "99254     1170\n",
      "99231     1078\n",
      "99223     1035\n",
      "90935     1027\n",
      "99222      970\n",
      "99253      865\n",
      "99255      700\n",
      "36620      661\n",
      "99238      485\n",
      "31624      469\n",
      "76942      447\n",
      "33405      436\n",
      "76937      352\n",
      "31645      342\n",
      "99252      334\n",
      "31622      315\n",
      "99292      313\n",
      "99239      312\n",
      "99221      268\n",
      "90801      238\n",
      "62270      225\n",
      "31500      194\n",
      "01996      178\n",
      "99024      169\n",
      "90945      154\n",
      "90937      150\n",
      "36489      145\n",
      "32002      140\n",
      "49080      128\n",
      "61312      124\n",
      "99251      111\n",
      "43246      108\n",
      "33427      107\n",
      "93503      107\n",
      "31600      105\n",
      "33430       93\n",
      "33533       91\n",
      "92960       90\n",
      "47135       89\n",
      "33860       86\n",
      "54150       84\n",
      "32422       83\n",
      "27245       80\n",
      "32000       75\n",
      "Name: CPT_CD, dtype: int64\n",
      "Available sections:\n",
      "\n",
      " {'Emerging technology', 'Anesthesia', 'Surgery', 'Medicine', 'Radiology', 'Evaluation and management', 'Pathology and laboratory'}\n",
      "\n",
      "The length of the initial dataset was 52743 and the new dataset is 38695\n",
      "\n",
      "\n",
      "Value Counts for the original data:\n",
      "\n",
      " 94003    12871\n",
      "94002     8206\n",
      "99291     4758\n",
      "99232     2385\n",
      "99233     1924\n",
      "36556     1788\n",
      "99254     1170\n",
      "99231     1078\n",
      "90935     1027\n",
      "36620      661\n",
      "31624      469\n",
      "76942      447\n",
      "33405      436\n",
      "76937      352\n",
      "31645      342\n",
      "90801      238\n",
      "01996      178\n",
      "99024      169\n",
      "76604       57\n",
      "75940       32\n",
      "99141       23\n",
      "85060       22\n",
      "99144       19\n",
      "75989       19\n",
      "0256T        8\n",
      "Name: CPT_CD, dtype: int64\n",
      "Value Counts for the filtered data:\n",
      "\n",
      " 94003    12871\n",
      "94002     8206\n",
      "99291     4758\n",
      "99232     2385\n",
      "99233     1924\n",
      "36556     1788\n",
      "99254     1170\n",
      "99231     1078\n",
      "90935     1027\n",
      "36620      661\n",
      "31624      469\n",
      "76942      447\n",
      "33405      436\n",
      "76937      352\n",
      "31645      342\n",
      "90801      238\n",
      "01996      178\n",
      "99024      169\n",
      "76604       57\n",
      "75940       32\n",
      "99141       23\n",
      "85060       22\n",
      "99144       19\n",
      "75989       19\n",
      "0256T        8\n",
      "Name: CPT_CD, dtype: int64\n",
      "\n",
      "Here are the counts by section:\n",
      "\n",
      " SECTIONHEADER\n",
      "Anesthesia                   3\n",
      "Evaluation and management    5\n",
      "Medicine                     5\n",
      "Pathology and laboratory     1\n",
      "Radiology                    5\n",
      "Surgery                      5\n",
      "Name: CPT_CD, dtype: int64\n",
      "\n",
      "Here are the counts by CPT by section\n",
      "\n",
      ":\n",
      "Anesthesia 01996 178\n",
      "Anesthesia 99141 23\n",
      "Anesthesia 99144 19\n",
      "Evaluation and management 99231 1078\n",
      "Evaluation and management 99232 2385\n",
      "Evaluation and management 99233 1924\n",
      "Evaluation and management 99254 1170\n",
      "Evaluation and management 99291 4758\n",
      "Medicine 90801 238\n",
      "Medicine 90935 1027\n",
      "Medicine 94002 8206\n",
      "Medicine 94003 12871\n",
      "Medicine 99024 169\n",
      "Pathology and laboratory 85060 22\n",
      "Radiology 75940 32\n",
      "Radiology 75989 19\n",
      "Radiology 76604 57\n",
      "Radiology 76937 352\n",
      "Radiology 76942 447\n",
      "Surgery 31624 469\n",
      "Surgery 31645 342\n",
      "Surgery 33405 436\n",
      "Surgery 36556 1788\n",
      "Surgery 36620 661\n"
     ]
    }
   ],
   "source": [
    "# Filter the number of CPT occurrences by section and CPT\n",
    "filtered_df1 = cpt_count_filter(note_cpt, note_cpt, 2, 10)\n",
    "\n",
    "# Show available sections\n",
    "print('Available sections:\\n\\n', set(filtered_df1['SECTIONHEADER']))\n",
    "\n",
    "# Filter to a set amount of CPT codes for each section header - only here to make the code run faster when testing\n",
    "filtered_df2 = cpt_per_section_filter(filtered_df1, 5, all_sections=True)\n",
    "\n",
    "# Filter to those CPT codes that have at least 100 notes or more\n",
    "filtered_df_cpt = filter_df(filtered_df2, 10)\n",
    "\n",
    "# Filter total number of CPT codes in the dataset by section\n",
    "# filtered_df_cpt = filter_cpt_ct(filtered_df_cpt, 'Evaluation and management', 3)\n",
    "\n",
    "# Show some dataset stats\n",
    "show_section_counts(filtered_df_cpt)\n",
    "show_cpt_counts_by_section(filtered_df_cpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "pursuant-batch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SECTIONHEADER              CPT_CD\n",
       "Anesthesia                 01996       178\n",
       "                           99141        23\n",
       "                           99144        19\n",
       "Evaluation and management  99231      1078\n",
       "                           99232      2385\n",
       "                           99233      1924\n",
       "                           99254      1170\n",
       "                           99291      4758\n",
       "Medicine                   90801       238\n",
       "                           90935      1027\n",
       "                           94002      8206\n",
       "                           94003     12871\n",
       "                           99024       169\n",
       "Pathology and laboratory   85060        22\n",
       "Radiology                  75940        32\n",
       "                           75989        19\n",
       "                           76604        57\n",
       "                           76937       352\n",
       "                           76942       447\n",
       "Surgery                    31624       469\n",
       "                           31645       342\n",
       "                           33405       436\n",
       "                           36556      1788\n",
       "                           36620       661\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df_cpt[['SECTIONHEADER','CPT_CD']].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-memorial",
   "metadata": {},
   "source": [
    "# Filter the Data - ICD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-sterling",
   "metadata": {},
   "source": [
    "I filtered the CID codes so there wouldn't be any notes with less than 100 notes. I also filtered to the top 5 ICD-9 codes to increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "spoken-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_icd(df, threshold):\n",
    "\n",
    "    x = df['ICD9_CODE'].value_counts() > threshold\n",
    "    y = list(x[x == 1].index)\n",
    "\n",
    "    z = df[df['ICD9_CODE'].isin(y)]\n",
    "\n",
    "    return z\n",
    "\n",
    "def filter_icd_ct(df,ct):\n",
    "    top_codes = list(df['ICD9_CODE'].value_counts().head(ct).index)\n",
    "    df = df[df.ICD9_CODE.isin(top_codes)]\n",
    "    return df\n",
    "\n",
    "filtered_df_icd = filter_df_icd(note_icd, 100)\n",
    "filtered_df_icd = filter_icd_ct(filtered_df_icd, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "collective-membrane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ICD9_CODE\n",
       "41401        3463\n",
       "0389         1976\n",
       "41071        1719\n",
       "V3001        1390\n",
       "4241         1136\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df_icd[['ICD9_CODE']].value_counts()\n",
    "# filtered_df_icd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-september",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-fitness",
   "metadata": {},
   "source": [
    "This was another step that was completed to preprocess the data. Sections that were not relevant were removed, such as social history, family history, and medication. Other steps that were taken included lowercasing all letters, removing dates and location, replacing return characters, replacing punctuation, and replacing digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unable-minimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "<ipython-input-19-20c3e3db6ba4>:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_cpt['TEXT'] = clean_data(filtered_df_cpt['TEXT'], True)\n"
     ]
    }
   ],
   "source": [
    "def clean_data(text_series, remove_sections):\n",
    "  \n",
    "    # Remove topics\n",
    "    data = text_series.str.lower() # lowercase all letters\n",
    "    data = data.str.split(r'(\\n\\n)')\n",
    "    if remove_sections:\n",
    "        for row_num, value in enumerate(data):\n",
    "            text_chunks = [x.split(':', maxsplit=1) for x in value]\n",
    "            ls = []\n",
    "            for i, x in enumerate(text_chunks):\n",
    "                try:\n",
    "                    if x[0] != 'social history' or x[0] != 'family history' or 'medication' not in x[0]:\n",
    "                        ls.append(x[1])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            text_series.iloc[row_num] = ' '.join(ls)\n",
    "\n",
    "        \n",
    "    # Remove dates and locations\n",
    "    text_series = text_series.str.replace('\\[\\*\\*(.*?)\\*\\*\\]', ' ', regex=True)\n",
    "    \n",
    "    # Replace \\n \n",
    "    text_series = text_series.str.replace('\\\\n',' ', regex=True)  \n",
    "    \n",
    "    # Replace punctuation\n",
    "    text_series = text_series.str.replace('[' + string.punctuation + ']', ' ', regex=True)\n",
    "    \n",
    "    # Remove all digits\n",
    "    text_series = text_series.str.replace('\\d',' ', regex=True)\n",
    "    \n",
    "    # Replace plurals, endings with ing, endings with ed, endings with ly\n",
    "#     text_series = text_series.str.replace('s(?=\\s)', ' ', regex=True)\n",
    "#     text_series = text_series.str.replace('ing(?=\\s)', ' ', regex=True)\n",
    "#     text_series = text_series.str.replace('ed(?=\\s)', ' ', regex=True)\n",
    "#     text_series = text_series.str.replace('ly(?=\\s)', ' ', regex=True)\n",
    "    \n",
    "    return text_series\n",
    "\n",
    "# Update Text Column -----\n",
    "filtered_df_cpt['TEXT'] = clean_data(filtered_df_cpt['TEXT'], True)\n",
    "filtered_df_icd['TEXT'] = clean_data(filtered_df_icd['TEXT'], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-somewhere",
   "metadata": {},
   "source": [
    "# Select CPT Code Sections to Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-wilderness",
   "metadata": {},
   "source": [
    "Set the variable sections to the list of CPT code sections. Anything not in those sections will have the option to grouped together into a category called 'other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "documented-coordinate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anesthesia', 'Surgery', 'Medicine', 'Radiology', 'Evaluation and management', 'Pathology and laboratory']\n",
      "['Evaluation and management', 'Surgery', 'Medicine', 'Radiology']\n"
     ]
    }
   ],
   "source": [
    "sections = list(set(filtered_df_cpt['SECTIONHEADER']))\n",
    "print(sections)\n",
    "sections = ['Evaluation and management', 'Surgery', 'Medicine', 'Radiology']\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-brain",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-guinea",
   "metadata": {},
   "source": [
    "Define a function to split the CPT code data. Sets test dataset size to 30% and training to 70%. Also shuffles the data to remove any bias implicit in the data ordering. Finally, combines CPT sections not selected into an other category in order to increase the number of notes in that section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "double-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_dict = {} \n",
    "\n",
    "def split_stratify_df(df, sections, combine_others=False, all_=False):\n",
    "    \n",
    "    # Train test split for each section selected\n",
    "    if all_:\n",
    "        sections.clear()\n",
    "    else:\n",
    "        for i in sections:\n",
    "            \n",
    "            # Test and training\n",
    "            df_x = df[df['SECTIONHEADER'] == i]['TEXT'].values\n",
    "            df_y = df[df['SECTIONHEADER'] == i]['CPT_CD']\n",
    "            tt_dict['X_train_' + i], tt_dict['X_test_' + i], tt_dict['y_train_' + i], tt_dict['y_test_' + i], \\\n",
    "            tt_dict['index_train_' + i], tt_dict['index_test_' + i] = \\\n",
    "            train_test_split(df_x, df_y, range(len(df_y)), test_size = .3, random_state = 42, shuffle=True)\n",
    "            \n",
    "            # Validation\n",
    "            tt_dict['X_test_' + i], tt_dict['X_val_' + i], tt_dict['y_test_' + i], tt_dict['y_val_' + i], \\\n",
    "            tt_dict['index_test_' + i], tt_dict['index_val_' + i] = \\\n",
    "            train_test_split(tt_dict['X_test_' + i], tt_dict['y_test_' + i], range(len(tt_dict['y_test_' + i])), \\\n",
    "                             test_size = .05, random_state = 42, shuffle=True)\n",
    "            \n",
    "    # Group other sections not included in selection\n",
    "    if combine_others:\n",
    "        i = 'other'\n",
    "        other_sections = list(set(df['SECTIONHEADER']).difference(set(sections)))\n",
    "        sections.append(i)\n",
    "        df_x = df[df['SECTIONHEADER'].isin(other_sections)]['TEXT'].values\n",
    "        df_y = df[df['SECTIONHEADER'].isin(other_sections)]['CPT_CD']\n",
    "        \n",
    "        # Test and training\n",
    "        tt_dict['X_train_' + i], tt_dict['X_test_' + i], tt_dict['y_train_' + i], tt_dict['y_test_' + i], \\\n",
    "        tt_dict['index_train_' + i], tt_dict['index_test_' + i] = \\\n",
    "        train_test_split(df_x, df_y, range(len(df_y)), test_size = .3, random_state = 42, shuffle=True)\n",
    "        \n",
    "        # Validation\n",
    "        tt_dict['X_test_' + i], tt_dict['X_val_' + i], tt_dict['y_test_' + i], tt_dict['y_val_' + i], \\\n",
    "        tt_dict['index_test_' + i], tt_dict['index_val_' + i] = \\\n",
    "        train_test_split(tt_dict['X_test_' + i], tt_dict['y_test_' + i], range(len(tt_dict['y_test_' + i])), \\\n",
    "                         test_size = .05, random_state = 42, shuffle=True)\n",
    "        \n",
    "split_stratify_df(filtered_df_cpt, sections, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-witch",
   "metadata": {},
   "source": [
    "# Select ICD sections to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "exciting-circular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Evaluation and management', 'Surgery', 'Medicine', 'Radiology', 'other', 'icd']\n"
     ]
    }
   ],
   "source": [
    "sections.append('icd')\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-adobe",
   "metadata": {},
   "source": [
    "# Split the Data - ICD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "breathing-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test\n",
    "tt_dict['X_train_icd'], tt_dict['X_test_icd'] , tt_dict['y_train_icd'], tt_dict['y_test_icd'] = \\\n",
    "train_test_split(filtered_df_icd['TEXT'].values, filtered_df_icd['ICD9_CODE'], test_size = .3, random_state = 42, shuffle=True)\n",
    "\n",
    "# Test and Validation\n",
    "tt_dict['X_test_icd'], tt_dict['X_val_icd'] , tt_dict['y_test_icd'], tt_dict['y_val_icd'] = \\\n",
    "train_test_split(tt_dict['X_test_icd'], tt_dict['y_test_icd'], test_size = .05, random_state = 42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-removal",
   "metadata": {},
   "source": [
    "# Balance the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-supplement",
   "metadata": {},
   "source": [
    "After splitting the data, the data needs to be balanced due to the dataset imbalance. Datasets are both under and oversampled based on the 99th percentile of the largest CPT/ICD-9 code. This improve the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "quantitative-catalog",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and management\n",
      "New Balanced Record Count per feature: 3281\n",
      "Surgery\n",
      "New Balanced Record Count per feature: 1230\n",
      "Medicine\n",
      "New Balanced Record Count per feature: 8878\n",
      "Radiology\n",
      "New Balanced Record Count per feature: 306\n",
      "other\n",
      "New Balanced Record Count per feature: 122\n",
      "icd\n",
      "New Balanced Record Count per feature: 2397\n"
     ]
    }
   ],
   "source": [
    "def oversample_df(X_train, y_train, percentile):\n",
    "            \n",
    "    # Recombine the training dataset\n",
    "    x = pd.Series(X_train).reset_index(drop=True)\n",
    "    y = pd.Series(y_train).reset_index(drop=True)\n",
    "    training_df = pd.concat([x,y], axis=1, ignore_index=True)\n",
    "\n",
    "    # Check counts\n",
    "    df_cts = training_df.iloc[:,1].value_counts()\n",
    "    record_ct = round(np.percentile(df_cts, percentile))\n",
    "    print('New Balanced Record Count per feature: {}'.format(record_ct))\n",
    "    \n",
    "    # Create a list of CPT values\n",
    "    df = list(df_cts.index.values)\n",
    "\n",
    "    # Resample\n",
    "    minority_df = []\n",
    "    for i in df:\n",
    "        test_resampled = resample(training_df[training_df.iloc[:,1] == i], replace=True, n_samples=record_ct, random_state=123)\n",
    "        minority_df.append(test_resampled)\n",
    "    \n",
    "    # Create final dataframe\n",
    "    new_df = pd.concat(minority_df)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def balance(tt_dict, sections):\n",
    "    for section in sections:\n",
    "        print(section)\n",
    "        \n",
    "        # Running balance function\n",
    "        training_balanced = oversample_df(tt_dict['X_train_' + section], tt_dict['y_train_' + section] , 99)\n",
    "        \n",
    "        # Reassign balanced data\n",
    "        tt_dict['X_train_' + section] = np.array(training_balanced.iloc[:,0].values)\n",
    "        tt_dict['y_train_' + section] = np.array(training_balanced.iloc[:,1].values)\n",
    "\n",
    "# Run the functions\n",
    "balance(tt_dict, sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-comment",
   "metadata": {},
   "source": [
    "# Tokenize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-excess",
   "metadata": {},
   "source": [
    "To run the NLP model, the data needs to be tokenized, which means the words need to be counted for each document. The vectorizer used is the TF-IDF vectorizer. This vectorizer is defined by this equation: TF(t,d) * IDF(t), where t is term frequency, d is the number of times term t appears in a document and IDF stands for the inverse document frequency. IDF is defined as log (1 + n/1 + df9d,t) + 1. N is the number of documents and df is the document frequency of t. There is one vectorizer made for each CPT/ICD-9 section. There are 6 sections total, including the ICD section. The hyperparameters were tuned using grid search in one of the draft notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "determined-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words\n",
    "my_stop_words = list(set(stopwords.words('english'))) \\\n",
    "                + ['admission', 'date', 'sex'] \\\n",
    "                + ['needed', 'every', 'seen', 'weeks', 'please', 'ml', 'unit', 'small', 'year', 'old', 'cm', 'non', 'mm', 'however']\n",
    "                # Got the above from my top 100 most predictive words that I wanted to remove\n",
    "\n",
    "# Set Dictionary for vectorized words\n",
    "vectorized_words = {}\n",
    "    \n",
    "def vectorize_df(train_test_dict, sections):\n",
    "\n",
    "    for section in sections:\n",
    "        \n",
    "        # Import TfidfVectorizer\n",
    "        vectorized_words['tfidf_vectorizer_' + section] = TfidfVectorizer(stop_words=my_stop_words, max_df=.7, min_df = 2, sublinear_tf = True, ngram_range = (1, 2))\n",
    "\n",
    "        # Transform the training data\n",
    "        tfidf_train = vectorized_words['tfidf_vectorizer_' + section].fit_transform(train_test_dict['X_train_' + section])\n",
    "\n",
    "        # Transform the test data\n",
    "        tfidf_test = vectorized_words['tfidf_vectorizer_' + section].transform(train_test_dict['X_test_' + section])\n",
    "\n",
    "        # Add results to dictionary\n",
    "\n",
    "        vectorized_words['tfidf_train_' + section] = tfidf_train\n",
    "        vectorized_words['tfidf_test_' + section] = tfidf_test\n",
    "\n",
    "vectorize_df(tt_dict, sections)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-census",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-tutorial",
   "metadata": {},
   "source": [
    "Logistic regression was chosen after considering 6 ML models in total, these included: Decision Tree, Random Forest, XG Boost, Multinomail Naive Bayes, KNN, and One vs Rest Logistic Regression. The hyperparameters were tuned in another notebook using gridsearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beginning-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models_lr = {}\n",
    "\n",
    "def run_clf(train_test_dict, sections):\n",
    "\n",
    "    # Fit and check accuracy\n",
    "    for section in sections:\n",
    "        \n",
    "        # Use Naive Bayes model\n",
    "        clf = LogisticRegression(random_state=123, C=1, max_iter=25, solver='sag', n_jobs=-1)\n",
    "        \n",
    "        # Fit model\n",
    "        clf.fit(vectorized_words['tfidf_train_' + section], train_test_dict['y_train_' + section])\n",
    "        \n",
    "        # Save in dictionary\n",
    "        models_lr[section] = clf\n",
    "    \n",
    "run_clf(tt_dict, sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-detective",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-application",
   "metadata": {},
   "source": [
    "This function checked the overall accuracy for each of the 6 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "forty-treaty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and management\n",
      "0.3234108527131783\n",
      "Surgery\n",
      "0.597340930674264\n",
      "Medicine\n",
      "0.836502493765586\n",
      "Radiology\n",
      "0.6872586872586872\n",
      "other\n",
      "0.8405797101449275\n",
      "icd\n",
      "0.8891304347826087\n"
     ]
    }
   ],
   "source": [
    "# models_ls = [models_nb, models_rf, models_xg, models_knn, models_lr, models_tree]\n",
    "models_ls = [models_lr]\n",
    "\n",
    "# Check accuracy\n",
    "def predictions(tt_dict, models, section=[]):\n",
    "    for key, model in models.items():\n",
    "        if len(section) == 0 or key == section:\n",
    "            pred = model.predict(vectorized_words['tfidf_test_' + key])\n",
    "            print(key)\n",
    "            print(metrics.accuracy_score(tt_dict['y_test_' + key], pred))\n",
    "\n",
    "predictions(tt_dict, models_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-kenya",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-canada",
   "metadata": {},
   "source": [
    "The classification report can be run for each model to find the precision, recall, and f-1 score to measure the models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "useful-strand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       01996       0.89      0.94      0.92        52\n",
      "       85060       0.80      0.57      0.67         7\n",
      "       99141       0.67      0.50      0.57         4\n",
      "       99144       0.50      0.50      0.50         6\n",
      "\n",
      "    accuracy                           0.84        69\n",
      "   macro avg       0.71      0.63      0.66        69\n",
      "weighted avg       0.83      0.84      0.83        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create classification report taken from here: https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "print('Test')\n",
    "section = 'other'\n",
    "pred = models_lr[section].predict(vectorized_words['tfidf_test_' + section])\n",
    "print(classification_report(tt_dict['y_test_' + section], pred))\n",
    "\n",
    "# print('Training')\n",
    "# pred_x = nb_classifier.predict(vectorized_words['tfidf_train'])\n",
    "# print(classification_report(tt_dict['y_train_' + section], pred_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-mining",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-closure",
   "metadata": {},
   "source": [
    "Predictions can be made using each model. The prediction probability was tested to be used in the front-end of the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "similar-senator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0389'] 96.32%\n",
      "0389\n"
     ]
    }
   ],
   "source": [
    "def predict_cpt(text, section):\n",
    "    input_text_clean = clean_data(pd.Series(text), False)\n",
    "    tfidf_input_test = vectorized_words['tfidf_vectorizer_' + section].transform(input_text_clean)\n",
    "    clf = models_lr[section]\n",
    "    \n",
    "    clf.predict(tfidf_input_test)\n",
    "    print(clf.predict(tfidf_input_test), str(round(max(clf.predict_proba(tfidf_input_test)[0]) * 100,2)) + '%')\n",
    "#     print(clf.predict_proba(tfidf_input_test)[0])\n",
    "\n",
    "# Set Variables\n",
    "txt_record_no = 1\n",
    "section = 'icd'\n",
    "\n",
    "# Run Function\n",
    "predict_cpt(tt_dict['X_val_' + section][txt_record_no], section)\n",
    "\n",
    "# print(np.array(tt_dict['y_val_' + section])[txt_record_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-swing",
   "metadata": {},
   "source": [
    "# Look at the Most/Least Predictive Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-forge",
   "metadata": {},
   "source": [
    "The top 100 and bottom 100 features are saved into the features_df dataframe. These are saved to csv files and later used in the front-end of the app using streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "spoken-adobe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 & Section: Evaluation and management\n",
      "Class 1 & Section: Evaluation and management\n",
      "Class 2 & Section: Evaluation and management\n",
      "Class 3 & Section: Evaluation and management\n",
      "Class 4 & Section: Evaluation and management\n",
      "Class 0 & Section: Surgery\n",
      "Class 1 & Section: Surgery\n",
      "Class 2 & Section: Surgery\n",
      "Class 3 & Section: Surgery\n",
      "Class 4 & Section: Surgery\n",
      "Class 0 & Section: Medicine\n",
      "Class 1 & Section: Medicine\n",
      "Class 2 & Section: Medicine\n",
      "Class 3 & Section: Medicine\n",
      "Class 4 & Section: Medicine\n",
      "Class 0 & Section: Radiology\n",
      "Class 1 & Section: Radiology\n",
      "Class 2 & Section: Radiology\n",
      "Class 3 & Section: Radiology\n",
      "Class 4 & Section: Radiology\n",
      "Class 0 & Section: other\n",
      "Class 1 & Section: other\n",
      "Class 2 & Section: other\n",
      "Class 3 & Section: other\n",
      "Class 0 & Section: icd\n",
      "Class 1 & Section: icd\n",
      "Class 2 & Section: icd\n",
      "Class 3 & Section: icd\n",
      "Class 4 & Section: icd\n"
     ]
    }
   ],
   "source": [
    "def get_features(sections):\n",
    "\n",
    "    # Initialize dataframe list\n",
    "    df_ls = []\n",
    "    \n",
    "    for section in sections: \n",
    "    \n",
    "        # Loop through for each class\n",
    "        for index, class_ in enumerate(models_lr[section].classes_):\n",
    "            \n",
    "            print('Class ' + str(index) + ' & Section: ' + section)\n",
    "\n",
    "            # Get the feature names\n",
    "            feature_names = vectorized_words['tfidf_vectorizer_' + section].get_feature_names()\n",
    "\n",
    "            # Get the probabilities\n",
    "            # Source: # https://sebastiansauer.github.io/convert_logit2prob/ for converting odds to log odds\n",
    "            probs = [np.exp(x)/(1 + np.exp(x)) for x in models_lr[section].coef_[index]]\n",
    "\n",
    "            # Zip together the first CPT weights with feature names\n",
    "            feat_with_weights =  sorted(zip(probs, feature_names))\n",
    "            feat_with_weights_r = feat_with_weights[::-1]\n",
    "\n",
    "            # Bottom 100 dataframe\n",
    "            bottom_100 = pd.DataFrame(feat_with_weights[:100], columns = ['Prob','Features'])\n",
    "            bottom_100['Class'] = models_lr[section].classes_[index]\n",
    "            bottom_100['Direction'] = 'Bottom'\n",
    "            bottom_100['Section'] = section\n",
    "\n",
    "            # Top 100 dataframe\n",
    "            top_100 = pd.DataFrame(feat_with_weights_r[:100], columns = ['Prob','Features'])\n",
    "            top_100['Class'] = models_lr[section].classes_[index]\n",
    "            top_100['Direction'] = 'Top'\n",
    "            top_100['Section'] = section\n",
    "\n",
    "            # Add dataframes to list\n",
    "            df_ls.append(bottom_100)\n",
    "            df_ls.append(top_100)\n",
    "\n",
    "    return pd.concat(df_ls)\n",
    "\n",
    "features_df = get_features(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "driven-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features\n",
    "features_df.to_csv('features_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-theology",
   "metadata": {},
   "source": [
    "# Save the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-colleague",
   "metadata": {},
   "source": [
    "Each model is saved for later use in the front-end Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dimensional-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from here: https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "# save the model to disk\n",
    "\n",
    "for key, model in models_lr.items():\n",
    "    filename = 'finalized_model_' + key + '.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-shield",
   "metadata": {},
   "source": [
    "# Save the Fitted Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-bride",
   "metadata": {},
   "source": [
    "The vectorizers are saved for later use in the front-end Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "broad-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, vectorizer in vectorized_words.items():\n",
    "    if 'vectorizer' in key:\n",
    "        filename = 'finalized_vectorizer_' + key + '.sav'\n",
    "        pickle.dump(vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-wheel",
   "metadata": {},
   "source": [
    "# Save the Text Sample Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-colombia",
   "metadata": {},
   "source": [
    "The text sample files are saved to use in the Streamlit app to generate examples of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "romantic-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in tt_dict.items():\n",
    "    if 'X_val_' in key or 'y_val_' in key:\n",
    "        pd.Series(value).to_csv(key + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
