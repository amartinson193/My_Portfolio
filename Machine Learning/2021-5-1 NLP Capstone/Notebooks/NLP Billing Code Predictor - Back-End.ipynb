{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expired-sharing",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "answering-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "from sklearn.utils import resample\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-alabama",
   "metadata": {},
   "source": [
    "\n",
    "# Import the data from MIMIC-III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-consultation",
   "metadata": {},
   "source": [
    "Importing 8 tables from the MIMIC-III database, all of them are related to ICD and CPT codes and the related clinical notes. I store this information in a dictionary that I can pull from later on in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "environmental-character",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (4,5,7,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "dataset_dictionary = {}\n",
    "\n",
    "for file_path in glob.glob('..\\\\Data\\\\MIMIC Files\\*'):\n",
    "    file_name = file_path.split('\\\\')[3].split('.')[0]\n",
    "    with gzip.open(file_path, mode='r') as file:\n",
    "        dataset_dictionary[file_name] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naked-video",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CPTEVENTS', 'DIAGNOSES_ICD', 'D_CPT', 'D_ICD_DIAGNOSES', 'D_ICD_PROCEDURES', 'NOTEEVENTS', 'PATIENTS', 'PROCEDURES_ICD'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dictionary.keys()\n",
    "# dataset_dictionary['D_ICD_DIAGNOSES']\n",
    "# dataset_dictionary['D_CPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "undefined-incentive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>COSTCENTER</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CPT_CD</th>\n",
       "      <th>CPT_NUMBER</th>\n",
       "      <th>CPT_SUFFIX</th>\n",
       "      <th>TICKET_ID_SEQ</th>\n",
       "      <th>SECTIONHEADER</th>\n",
       "      <th>SUBSECTIONHEADER</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317</td>\n",
       "      <td>11743</td>\n",
       "      <td>129545</td>\n",
       "      <td>ICU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99232</td>\n",
       "      <td>99232.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Evaluation and management</td>\n",
       "      <td>Hospital inpatient services</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>318</td>\n",
       "      <td>11743</td>\n",
       "      <td>129545</td>\n",
       "      <td>ICU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99232</td>\n",
       "      <td>99232.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Evaluation and management</td>\n",
       "      <td>Hospital inpatient services</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>319</td>\n",
       "      <td>11743</td>\n",
       "      <td>129545</td>\n",
       "      <td>ICU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99232</td>\n",
       "      <td>99232.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Evaluation and management</td>\n",
       "      <td>Hospital inpatient services</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>320</td>\n",
       "      <td>11743</td>\n",
       "      <td>129545</td>\n",
       "      <td>ICU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99232</td>\n",
       "      <td>99232.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Evaluation and management</td>\n",
       "      <td>Hospital inpatient services</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>6185</td>\n",
       "      <td>183725</td>\n",
       "      <td>ICU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99223</td>\n",
       "      <td>99223.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Evaluation and management</td>\n",
       "      <td>Hospital inpatient services</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573141</th>\n",
       "      <td>573142</td>\n",
       "      <td>78876</td>\n",
       "      <td>163404</td>\n",
       "      <td>Resp</td>\n",
       "      <td>2105-09-01 00:00:00</td>\n",
       "      <td>94003</td>\n",
       "      <td>94003.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>Pulmonary</td>\n",
       "      <td>VENT MGMT;SUBSQ DAYS(INVASIVE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573142</th>\n",
       "      <td>573143</td>\n",
       "      <td>78879</td>\n",
       "      <td>136071</td>\n",
       "      <td>Resp</td>\n",
       "      <td>2150-08-29 00:00:00</td>\n",
       "      <td>94003</td>\n",
       "      <td>94003.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>Pulmonary</td>\n",
       "      <td>VENT MGMT;SUBSQ DAYS(INVASIVE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573143</th>\n",
       "      <td>573144</td>\n",
       "      <td>78879</td>\n",
       "      <td>136071</td>\n",
       "      <td>Resp</td>\n",
       "      <td>2150-08-28 00:00:00</td>\n",
       "      <td>94002</td>\n",
       "      <td>94002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>Pulmonary</td>\n",
       "      <td>VENT MGMT, 1ST DAY (INVASIVE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573144</th>\n",
       "      <td>573145</td>\n",
       "      <td>78892</td>\n",
       "      <td>175171</td>\n",
       "      <td>Resp</td>\n",
       "      <td>2125-06-11 00:00:00</td>\n",
       "      <td>94003</td>\n",
       "      <td>94003.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>Pulmonary</td>\n",
       "      <td>VENT MGMT;SUBSQ DAYS(INVASIVE)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573145</th>\n",
       "      <td>573146</td>\n",
       "      <td>78892</td>\n",
       "      <td>175171</td>\n",
       "      <td>Resp</td>\n",
       "      <td>2125-06-10 00:00:00</td>\n",
       "      <td>94003</td>\n",
       "      <td>94003.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>Pulmonary</td>\n",
       "      <td>VENT MGMT;SUBSQ DAYS(INVASIVE)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573146 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ROW_ID  SUBJECT_ID  HADM_ID COSTCENTER            CHARTDATE CPT_CD  \\\n",
       "0          317       11743   129545        ICU                  NaN  99232   \n",
       "1          318       11743   129545        ICU                  NaN  99232   \n",
       "2          319       11743   129545        ICU                  NaN  99232   \n",
       "3          320       11743   129545        ICU                  NaN  99232   \n",
       "4          321        6185   183725        ICU                  NaN  99223   \n",
       "...        ...         ...      ...        ...                  ...    ...   \n",
       "573141  573142       78876   163404       Resp  2105-09-01 00:00:00  94003   \n",
       "573142  573143       78879   136071       Resp  2150-08-29 00:00:00  94003   \n",
       "573143  573144       78879   136071       Resp  2150-08-28 00:00:00  94002   \n",
       "573144  573145       78892   175171       Resp  2125-06-11 00:00:00  94003   \n",
       "573145  573146       78892   175171       Resp  2125-06-10 00:00:00  94003   \n",
       "\n",
       "        CPT_NUMBER CPT_SUFFIX  TICKET_ID_SEQ              SECTIONHEADER  \\\n",
       "0          99232.0        NaN            6.0  Evaluation and management   \n",
       "1          99232.0        NaN            7.0  Evaluation and management   \n",
       "2          99232.0        NaN            8.0  Evaluation and management   \n",
       "3          99232.0        NaN            9.0  Evaluation and management   \n",
       "4          99223.0        NaN            1.0  Evaluation and management   \n",
       "...            ...        ...            ...                        ...   \n",
       "573141     94003.0        NaN            NaN                   Medicine   \n",
       "573142     94003.0        NaN            NaN                   Medicine   \n",
       "573143     94002.0        NaN            NaN                   Medicine   \n",
       "573144     94003.0        NaN            NaN                   Medicine   \n",
       "573145     94003.0        NaN            NaN                   Medicine   \n",
       "\n",
       "                   SUBSECTIONHEADER                     DESCRIPTION  \n",
       "0       Hospital inpatient services                             NaN  \n",
       "1       Hospital inpatient services                             NaN  \n",
       "2       Hospital inpatient services                             NaN  \n",
       "3       Hospital inpatient services                             NaN  \n",
       "4       Hospital inpatient services                             NaN  \n",
       "...                             ...                             ...  \n",
       "573141                    Pulmonary  VENT MGMT;SUBSQ DAYS(INVASIVE)  \n",
       "573142                    Pulmonary  VENT MGMT;SUBSQ DAYS(INVASIVE)  \n",
       "573143                    Pulmonary   VENT MGMT, 1ST DAY (INVASIVE)  \n",
       "573144                    Pulmonary  VENT MGMT;SUBSQ DAYS(INVASIVE)  \n",
       "573145                    Pulmonary  VENT MGMT;SUBSQ DAYS(INVASIVE)  \n",
       "\n",
       "[573146 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dictionary['CPTEVENTS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-literature",
   "metadata": {},
   "source": [
    "# Assign Datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-cardiff",
   "metadata": {},
   "source": [
    "This section cleans up the code and fixes any datatype issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "delayed-moscow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CPTEVENTS', 'DIAGNOSES_ICD', 'D_CPT', 'D_ICD_DIAGNOSES', 'D_ICD_PROCEDURES', 'NOTEEVENTS', 'PATIENTS', 'PROCEDURES_ICD'])\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 573146 entries, 0 to 573145\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   ROW_ID            573146 non-null  int64  \n",
      " 1   SUBJECT_ID        573146 non-null  int64  \n",
      " 2   HADM_ID           573146 non-null  int64  \n",
      " 3   COSTCENTER        573146 non-null  object \n",
      " 4   CHARTDATE         101545 non-null  object \n",
      " 5   CPT_CD            573146 non-null  object \n",
      " 6   CPT_NUMBER        573128 non-null  float64\n",
      " 7   CPT_SUFFIX        22 non-null      object \n",
      " 8   TICKET_ID_SEQ     471601 non-null  float64\n",
      " 9   SECTIONHEADER     573125 non-null  object \n",
      " 10  SUBSECTIONHEADER  573125 non-null  object \n",
      " 11  DESCRIPTION       101545 non-null  object \n",
      "dtypes: float64(2), int64(3), object(7)\n",
      "memory usage: 52.5+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 651047 entries, 0 to 651046\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   ROW_ID      651047 non-null  int64  \n",
      " 1   SUBJECT_ID  651047 non-null  int64  \n",
      " 2   HADM_ID     651047 non-null  int64  \n",
      " 3   SEQ_NUM     651000 non-null  float64\n",
      " 4   ICD9_CODE   651000 non-null  object \n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 24.8+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 134 entries, 0 to 133\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   ROW_ID               134 non-null    int64 \n",
      " 1   CATEGORY             134 non-null    int64 \n",
      " 2   SECTIONRANGE         134 non-null    object\n",
      " 3   SECTIONHEADER        134 non-null    object\n",
      " 4   SUBSECTIONRANGE      134 non-null    object\n",
      " 5   SUBSECTIONHEADER     134 non-null    object\n",
      " 6   CODESUFFIX           11 non-null     object\n",
      " 7   MINCODEINSUBSECTION  134 non-null    int64 \n",
      " 8   MAXCODEINSUBSECTION  134 non-null    int64 \n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 9.5+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14567 entries, 0 to 14566\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ROW_ID       14567 non-null  int64 \n",
      " 1   ICD9_CODE    14567 non-null  object\n",
      " 2   SHORT_TITLE  14567 non-null  object\n",
      " 3   LONG_TITLE   14567 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 455.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3882 entries, 0 to 3881\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ROW_ID       3882 non-null   int64 \n",
      " 1   ICD9_CODE    3882 non-null   int64 \n",
      " 2   SHORT_TITLE  3882 non-null   object\n",
      " 3   LONG_TITLE   3882 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 121.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2083180 entries, 0 to 2083179\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   ROW_ID       int64  \n",
      " 1   SUBJECT_ID   int64  \n",
      " 2   HADM_ID      float64\n",
      " 3   CHARTDATE    object \n",
      " 4   CHARTTIME    object \n",
      " 5   STORETIME    object \n",
      " 6   CATEGORY     object \n",
      " 7   DESCRIPTION  object \n",
      " 8   CGID         float64\n",
      " 9   ISERROR      float64\n",
      " 10  TEXT         object \n",
      "dtypes: float64(3), int64(2), object(6)\n",
      "memory usage: 174.8+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46520 entries, 0 to 46519\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ROW_ID       46520 non-null  int64 \n",
      " 1   SUBJECT_ID   46520 non-null  int64 \n",
      " 2   GENDER       46520 non-null  object\n",
      " 3   DOB          46520 non-null  object\n",
      " 4   DOD          15759 non-null  object\n",
      " 5   DOD_HOSP     9974 non-null   object\n",
      " 6   DOD_SSN      13378 non-null  object\n",
      " 7   EXPIRE_FLAG  46520 non-null  int64 \n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 2.8+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 240095 entries, 0 to 240094\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype\n",
      "---  ------      --------------   -----\n",
      " 0   ROW_ID      240095 non-null  int64\n",
      " 1   SUBJECT_ID  240095 non-null  int64\n",
      " 2   HADM_ID     240095 non-null  int64\n",
      " 3   SEQ_NUM     240095 non-null  int64\n",
      " 4   ICD9_CODE   240095 non-null  int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 9.2 MB\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'to_datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1384668da6e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# CPTEVENTS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdataset_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CPTEVENTS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SECTIONHEADER'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CPT_CD'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CPTEVENTS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SECTIONHEADER'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CPT_CD'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdataset_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CPTEVENTS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CHARTDATE'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_dictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CPTEVENTS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CHARTDATE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5460\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5461\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5462\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5464\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'to_datetime'"
     ]
    }
   ],
   "source": [
    "# Check all the datasets exist in the dictionary \n",
    "print(dataset_dictionary.keys())\n",
    "\n",
    "# Check the datatypes and information for each table \n",
    "for i in dataset_dictionary.keys():\n",
    "    print(dataset_dictionary[i].info())\n",
    "\n",
    "# Correct any datatype issues #####\n",
    "\n",
    "# CPTEVENTS\n",
    "dataset_dictionary['CPTEVENTS'].loc[:,['SECTIONHEADER','CPT_CD']] = dataset_dictionary['CPTEVENTS'].loc[:,['SECTIONHEADER','CPT_CD']].astype(str)\n",
    "dataset_dictionary['CPTEVENTS']['CHARTDATE'] = dataset_dictionary['CPTEVENTS']['CHARTDATE'].to_datetime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-desert",
   "metadata": {},
   "source": [
    "# Join the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-sailing",
   "metadata": {},
   "source": [
    "The ICD and CPT codes needed to be joined to the clinical notes. Also, a little bit of data cleaning needed to be done. The clinical notes were limited to just the discharge summaries since that helps to capture an overview of the patient encounter. The original number of clinical notes was 2,083,180 and filtering the category to discharge summary reduced it to 223,571 notes. The description type is also filtered to just report. The joins were made on the subject ID and HADM ID (encounter ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "grateful-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tables(dataset_dictionary, category=['Discharge summary'], all_notes=False):\n",
    "\n",
    "    # Define tables\n",
    "    note_events_base = dataset_dictionary['NOTEEVENTS']\n",
    "    cpt_events_base = dataset_dictionary['CPTEVENTS']\n",
    "    icd_events_base = dataset_dictionary['DIAGNOSES_ICD']\n",
    "\n",
    "    # Combine text for each subject and encounter\n",
    "    if all_notes == False:\n",
    "        note_events_base = note_events_base[note_events_base.loc[:,'CATEGORY'].isin(category)]\n",
    "    \n",
    "    # Filter out the addendums and restrict notes only to reports\n",
    "    note_events_base = note_events_base[note_events_base['DESCRIPTION'] == 'Report']\n",
    "    \n",
    "    # Aggregate text by Subject and HADM ID's\n",
    "    note_events = note_events_base.groupby(['SUBJECT_ID', 'HADM_ID'], as_index=False)['TEXT'].agg(sum)\n",
    "    \n",
    "    # Create CPT table\n",
    "    cpt_events_base = cpt_events_base.loc[:, ['SUBJECT_ID','HADM_ID', 'CPT_CD', 'SECTIONHEADER', 'DESCRIPTION']]\n",
    "    cpt_events = cpt_events_base.drop_duplicates()\n",
    "    \n",
    "    # Create ICD table\n",
    "    icd_events_base = icd_events_base[icd_events_base['SEQ_NUM'] == 1]\n",
    "    icd_events_base = icd_events_base.loc[:, ['SUBJECT_ID','HADM_ID', 'ICD9_CODE']]\n",
    "    icd_events = icd_events_base.drop_duplicates()\n",
    "    \n",
    "    # Join the datasets\n",
    "    note_cpt = note_events.merge(cpt_events, on = ['SUBJECT_ID','HADM_ID'])\n",
    "    note_icd = note_events.merge(icd_events, on = ['SUBJECT_ID', 'HADM_ID'])\n",
    "    \n",
    "    # Replace any nulls with blanks\n",
    "    x = note_cpt[note_cpt['DESCRIPTION'].isnull()].copy()\n",
    "    x.loc[:,'DESCRIPTION'] = ''\n",
    "    y = note_cpt[note_cpt['DESCRIPTION'].notnull()].copy()\n",
    "    note_cpt = pd.concat([x,y])\n",
    "    \n",
    "    # Combine description and text columns\n",
    "    note_cpt['TEXT'] = note_cpt['TEXT'] + note_cpt['DESCRIPTION']\n",
    "    note_cpt = note_cpt.drop('DESCRIPTION', axis=1)\n",
    "    \n",
    "    return note_cpt, note_icd\n",
    "\n",
    "# Run the function\n",
    "note_cpt, note_icd = join_tables(dataset_dictionary)\n",
    "\n",
    "# Drop notes with the nan sectionheader\n",
    "drop_ls = note_cpt[note_cpt['SECTIONHEADER'] == 'nan']\n",
    "note_cpt = note_cpt.drop(drop_ls.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "agricultural-david",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>DOB</th>\n",
       "      <th>DOD</th>\n",
       "      <th>DOD_HOSP</th>\n",
       "      <th>DOD_SSN</th>\n",
       "      <th>EXPIRE_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>234</td>\n",
       "      <td>249</td>\n",
       "      <td>F</td>\n",
       "      <td>2075-03-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>235</td>\n",
       "      <td>250</td>\n",
       "      <td>F</td>\n",
       "      <td>2164-12-27 00:00:00</td>\n",
       "      <td>2188-11-22 00:00:00</td>\n",
       "      <td>2188-11-22 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>236</td>\n",
       "      <td>251</td>\n",
       "      <td>M</td>\n",
       "      <td>2090-03-15 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>237</td>\n",
       "      <td>252</td>\n",
       "      <td>M</td>\n",
       "      <td>2078-03-06 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>238</td>\n",
       "      <td>253</td>\n",
       "      <td>F</td>\n",
       "      <td>2089-11-26 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46515</th>\n",
       "      <td>31840</td>\n",
       "      <td>44089</td>\n",
       "      <td>M</td>\n",
       "      <td>2026-05-25 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46516</th>\n",
       "      <td>31841</td>\n",
       "      <td>44115</td>\n",
       "      <td>F</td>\n",
       "      <td>2124-07-27 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46517</th>\n",
       "      <td>31842</td>\n",
       "      <td>44123</td>\n",
       "      <td>F</td>\n",
       "      <td>2049-11-26 00:00:00</td>\n",
       "      <td>2135-01-12 00:00:00</td>\n",
       "      <td>2135-01-12 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46518</th>\n",
       "      <td>31843</td>\n",
       "      <td>44126</td>\n",
       "      <td>F</td>\n",
       "      <td>2076-07-25 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46519</th>\n",
       "      <td>31844</td>\n",
       "      <td>44128</td>\n",
       "      <td>M</td>\n",
       "      <td>2098-07-25 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46520 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROW_ID  SUBJECT_ID GENDER                  DOB                  DOD  \\\n",
       "0         234         249      F  2075-03-13 00:00:00                  NaN   \n",
       "1         235         250      F  2164-12-27 00:00:00  2188-11-22 00:00:00   \n",
       "2         236         251      M  2090-03-15 00:00:00                  NaN   \n",
       "3         237         252      M  2078-03-06 00:00:00                  NaN   \n",
       "4         238         253      F  2089-11-26 00:00:00                  NaN   \n",
       "...       ...         ...    ...                  ...                  ...   \n",
       "46515   31840       44089      M  2026-05-25 00:00:00                  NaN   \n",
       "46516   31841       44115      F  2124-07-27 00:00:00                  NaN   \n",
       "46517   31842       44123      F  2049-11-26 00:00:00  2135-01-12 00:00:00   \n",
       "46518   31843       44126      F  2076-07-25 00:00:00                  NaN   \n",
       "46519   31844       44128      M  2098-07-25 00:00:00                  NaN   \n",
       "\n",
       "                  DOD_HOSP DOD_SSN  EXPIRE_FLAG  \n",
       "0                      NaN     NaN            0  \n",
       "1      2188-11-22 00:00:00     NaN            1  \n",
       "2                      NaN     NaN            0  \n",
       "3                      NaN     NaN            0  \n",
       "4                      NaN     NaN            0  \n",
       "...                    ...     ...          ...  \n",
       "46515                  NaN     NaN            0  \n",
       "46516                  NaN     NaN            0  \n",
       "46517  2135-01-12 00:00:00     NaN            1  \n",
       "46518                  NaN     NaN            0  \n",
       "46519                  NaN     NaN            0  \n",
       "\n",
       "[46520 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dictionary['PATIENTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specified-taste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>CPT_CD</th>\n",
       "      <th>SECTIONHEADER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>185777.0</td>\n",
       "      <td>Admission Date:  [**2191-3-16**]     Discharge...</td>\n",
       "      <td>99223</td>\n",
       "      <td>Evaluation and management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>185777.0</td>\n",
       "      <td>Admission Date:  [**2191-3-16**]     Discharge...</td>\n",
       "      <td>99233</td>\n",
       "      <td>Evaluation and management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>185777.0</td>\n",
       "      <td>Admission Date:  [**2191-3-16**]     Discharge...</td>\n",
       "      <td>99232</td>\n",
       "      <td>Evaluation and management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>185777.0</td>\n",
       "      <td>Admission Date:  [**2191-3-16**]     Discharge...</td>\n",
       "      <td>99231</td>\n",
       "      <td>Evaluation and management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>185777.0</td>\n",
       "      <td>Admission Date:  [**2191-3-16**]     Discharge...</td>\n",
       "      <td>99238</td>\n",
       "      <td>Evaluation and management</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID   HADM_ID                                               TEXT  \\\n",
       "2           4  185777.0  Admission Date:  [**2191-3-16**]     Discharge...   \n",
       "3           4  185777.0  Admission Date:  [**2191-3-16**]     Discharge...   \n",
       "4           4  185777.0  Admission Date:  [**2191-3-16**]     Discharge...   \n",
       "5           4  185777.0  Admission Date:  [**2191-3-16**]     Discharge...   \n",
       "6           4  185777.0  Admission Date:  [**2191-3-16**]     Discharge...   \n",
       "\n",
       "  CPT_CD              SECTIONHEADER  \n",
       "2  99223  Evaluation and management  \n",
       "3  99233  Evaluation and management  \n",
       "4  99232  Evaluation and management  \n",
       "5  99231  Evaluation and management  \n",
       "6  99238  Evaluation and management  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_cpt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-dealing",
   "metadata": {},
   "source": [
    "# Filter the data - CPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-spiritual",
   "metadata": {},
   "source": [
    "A number of different functions were made which helped to filter the CPT codes. This was part of the preprocessing process and the steps that were made here improved accuracy in the model a lot. In the end, I partitioned the CPT codes by the CPT Category Code Level 1. There are 6 sections. I took the top 5 from each one. I also filtered to only codes that had 100 notes or more to help the machine learning model learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pursuant-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(combined_df, threshold):\n",
    "\n",
    "    # Print value counts original\n",
    "    print('Value Counts for the original data:\\n\\n', combined_df['CPT_CD'].value_counts().head(25))\n",
    "\n",
    "    # Filter based on count limit\n",
    "    df = combined_df['CPT_CD'].value_counts()\n",
    "    filtered_ls = list((df[df >= threshold]).index.values)\n",
    "    filtered_df = combined_df[combined_df['CPT_CD'].isin(filtered_ls)]\n",
    "    \n",
    "    # Print value counts filtered\n",
    "    print('Value Counts for the filtered data:\\n\\n', combined_df['CPT_CD'].value_counts().head(25))\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Find Counts of CPT Codes per Patient Encounter and filter df\n",
    "def cpt_count_filter(df, og_df, cpt_section_hadm_limit, cpt_hadm_limit):\n",
    "    \n",
    "    # Filter based on limit per SECTIONHEADER & merge\n",
    "    df1 = og_df.groupby(['HADM_ID', 'SECTIONHEADER'])['CPT_CD'].count()\n",
    "    filtered_encntrs = df1[df1 <= cpt_section_hadm_limit]\n",
    "    final_df = df.merge(filtered_encntrs, on=['HADM_ID', 'SECTIONHEADER'])\n",
    "    final_df.drop('CPT_CD_y', axis=1, inplace=True)\n",
    "    \n",
    "    # Filter dataset again based on total number of CPT codes per HADM\n",
    "    df2 = og_df.groupby(['HADM_ID'])['CPT_CD'].count()\n",
    "    filtered_encntrs = df2[df2 <= cpt_hadm_limit]\n",
    "    final_df = final_df.merge(filtered_encntrs, on=['HADM_ID'])\n",
    "    final_df.drop('CPT_CD', axis=1, inplace=True)\n",
    "    \n",
    "    # Rename columns\n",
    "    final_df.columns = ['SUBJECT_ID', 'HADM_ID', 'TEXT', 'CPT_CD', 'SECTIONHEADER']\n",
    "    \n",
    "    print('Value Counts for the filtered data:\\n\\n', final_df['CPT_CD'].value_counts().head(50))\n",
    "\n",
    "    return final_df\n",
    "    \n",
    "# Filter DataFrame to a set amount of CPT codes in each section header #####\n",
    "def cpt_per_section_filter(df, section_limit, sections=['Emerging technology'], all_sections=False):\n",
    "    \n",
    "    # Create list for dataframes\n",
    "    df_ls = []\n",
    "\n",
    "    # Group by and count the number of CPT codes\n",
    "    cts_by_cpt = df.groupby(['SECTIONHEADER', 'CPT_CD'])['CPT_CD'].count()\n",
    "    cts_by_cpt.index.names = ['SECTIONHEADER', 'CPT_CDS']\n",
    "    cts_by_cpt = cts_by_cpt.reset_index()\n",
    "    cts_by_cpt.columns = ['SECTIONHEADER', 'CPT_CD', 'COUNT']\n",
    "\n",
    "    # Sort values by section and CPT code count\n",
    "    cts_by_cpt_s = cts_by_cpt.sort_values(by=['SECTIONHEADER','COUNT'], ascending=False)\n",
    "\n",
    "    # Filter based on the limit of CPT codes wanted for each category\n",
    "    if all_sections == True:\n",
    "        sections = list(set(df['SECTIONHEADER']))\n",
    "        \n",
    "    for i in sections:\n",
    "        top_cts = cts_by_cpt_s[cts_by_cpt_s['SECTIONHEADER'] == i].iloc[:section_limit,:]\n",
    "\n",
    "        # Append to list\n",
    "        df_ls.append(top_cts)\n",
    "\n",
    "    # Combine DataFrames\n",
    "    df_combo = pd.concat(df_ls)\n",
    "\n",
    "    # Join back to source data\n",
    "    final_df = df.merge(df_combo, on=['SECTIONHEADER','CPT_CD'])\n",
    "    \n",
    "    print('\\nThe length of the initial dataset was {} and the new dataset is {}\\n\\n'.format(len(df), len(final_df)))\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def show_section_counts(df):\n",
    "    # Print count of CPT codes by section\n",
    "    cts_by_cpt = df.groupby(['SECTIONHEADER', 'CPT_CD'])['CPT_CD'].count()\n",
    "    cts_by_section = cts_by_cpt.groupby('SECTIONHEADER').count()\n",
    "    print('\\nHere are the counts by section:\\n\\n', cts_by_section)\n",
    "\n",
    "def show_cpt_counts_by_section(df):\n",
    "    cts_by_cpt = df.groupby(['SECTIONHEADER', 'CPT_CD'])['CPT_CD'].count()\n",
    "    print('\\nHere are the counts by CPT by section\\n\\n:')\n",
    "#     print(pd.DataFrame(cts_by_cpt.rename('Count')).reset_index())\n",
    "    for i, x in pd.DataFrame(cts_by_cpt.rename('Count')).reset_index().iterrows():\n",
    "        print(x['SECTIONHEADER'], x['CPT_CD'], x['Count'])\n",
    "        \n",
    "def filter_cpt_ct(df, section, ct):\n",
    "    df = df[df['SECTIONHEADER'] == section]\n",
    "    top_codes = list(df['CPT_CD'].value_counts().head(ct).index)\n",
    "    df = df[df.CPT_CD.isin(top_codes)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "encouraging-music",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Counts for the filtered data:\n",
      "\n",
      " 94003    12871\n",
      "94002     8206\n",
      "99291     4758\n",
      "99232     2385\n",
      "99233     1924\n",
      "36556     1788\n",
      "99254     1170\n",
      "99231     1078\n",
      "99223     1035\n",
      "90935     1027\n",
      "99222      970\n",
      "99253      865\n",
      "99255      700\n",
      "36620      661\n",
      "99238      485\n",
      "31624      469\n",
      "76942      447\n",
      "33405      436\n",
      "76937      352\n",
      "31645      342\n",
      "99252      334\n",
      "31622      315\n",
      "99292      313\n",
      "99239      312\n",
      "99221      268\n",
      "90801      238\n",
      "62270      225\n",
      "31500      194\n",
      "01996      178\n",
      "99024      169\n",
      "90945      154\n",
      "90937      150\n",
      "36489      145\n",
      "32002      140\n",
      "49080      128\n",
      "61312      124\n",
      "99251      111\n",
      "43246      108\n",
      "33427      107\n",
      "93503      107\n",
      "31600      105\n",
      "33430       93\n",
      "33533       91\n",
      "92960       90\n",
      "47135       89\n",
      "33860       86\n",
      "54150       84\n",
      "32422       83\n",
      "27245       80\n",
      "32000       75\n",
      "Name: CPT_CD, dtype: int64\n",
      "Available sections:\n",
      "\n",
      " {'Medicine', 'Radiology', 'Evaluation and management', 'Emerging technology', 'Pathology and laboratory', 'Surgery', 'Anesthesia'}\n",
      "\n",
      "The length of the initial dataset was 52743 and the new dataset is 38695\n",
      "\n",
      "\n",
      "Value Counts for the original data:\n",
      "\n",
      " 94003    12871\n",
      "94002     8206\n",
      "99291     4758\n",
      "99232     2385\n",
      "99233     1924\n",
      "36556     1788\n",
      "99254     1170\n",
      "99231     1078\n",
      "90935     1027\n",
      "36620      661\n",
      "31624      469\n",
      "76942      447\n",
      "33405      436\n",
      "76937      352\n",
      "31645      342\n",
      "90801      238\n",
      "01996      178\n",
      "99024      169\n",
      "76604       57\n",
      "75940       32\n",
      "99141       23\n",
      "85060       22\n",
      "99144       19\n",
      "75989       19\n",
      "0256T        8\n",
      "Name: CPT_CD, dtype: int64\n",
      "Value Counts for the filtered data:\n",
      "\n",
      " 94003    12871\n",
      "94002     8206\n",
      "99291     4758\n",
      "99232     2385\n",
      "99233     1924\n",
      "36556     1788\n",
      "99254     1170\n",
      "99231     1078\n",
      "90935     1027\n",
      "36620      661\n",
      "31624      469\n",
      "76942      447\n",
      "33405      436\n",
      "76937      352\n",
      "31645      342\n",
      "90801      238\n",
      "01996      178\n",
      "99024      169\n",
      "76604       57\n",
      "75940       32\n",
      "99141       23\n",
      "85060       22\n",
      "99144       19\n",
      "75989       19\n",
      "0256T        8\n",
      "Name: CPT_CD, dtype: int64\n",
      "\n",
      "Here are the counts by section:\n",
      "\n",
      " SECTIONHEADER\n",
      "Anesthesia                   3\n",
      "Evaluation and management    5\n",
      "Medicine                     5\n",
      "Pathology and laboratory     1\n",
      "Radiology                    5\n",
      "Surgery                      5\n",
      "Name: CPT_CD, dtype: int64\n",
      "\n",
      "Here are the counts by CPT by section\n",
      "\n",
      ":\n",
      "Anesthesia 01996 178\n",
      "Anesthesia 99141 23\n",
      "Anesthesia 99144 19\n",
      "Evaluation and management 99231 1078\n",
      "Evaluation and management 99232 2385\n",
      "Evaluation and management 99233 1924\n",
      "Evaluation and management 99254 1170\n",
      "Evaluation and management 99291 4758\n",
      "Medicine 90801 238\n",
      "Medicine 90935 1027\n",
      "Medicine 94002 8206\n",
      "Medicine 94003 12871\n",
      "Medicine 99024 169\n",
      "Pathology and laboratory 85060 22\n",
      "Radiology 75940 32\n",
      "Radiology 75989 19\n",
      "Radiology 76604 57\n",
      "Radiology 76937 352\n",
      "Radiology 76942 447\n",
      "Surgery 31624 469\n",
      "Surgery 31645 342\n",
      "Surgery 33405 436\n",
      "Surgery 36556 1788\n",
      "Surgery 36620 661\n"
     ]
    }
   ],
   "source": [
    "# Filter the number of CPT occurrences by section and CPT\n",
    "filtered_df1 = cpt_count_filter(note_cpt, note_cpt, 2, 10)\n",
    "\n",
    "# Show available sections\n",
    "print('Available sections:\\n\\n', set(filtered_df1['SECTIONHEADER']))\n",
    "\n",
    "# Filter to a set amount of CPT codes for each section header - only here to make the code run faster when testing\n",
    "filtered_df2 = cpt_per_section_filter(filtered_df1, 5, all_sections=True)\n",
    "\n",
    "# Filter to those CPT codes that have at least 10 notes or more\n",
    "filtered_df_cpt = filter_df(filtered_df2, 10)\n",
    "\n",
    "# Filter total number of CPT codes in the dataset by section\n",
    "# filtered_df_cpt = filter_cpt_ct(filtered_df_cpt, 'Evaluation and management', 3)\n",
    "\n",
    "# Show some dataset stats\n",
    "show_section_counts(filtered_df_cpt)\n",
    "show_cpt_counts_by_section(filtered_df_cpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "pursuant-batch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SECTIONHEADER              CPT_CD\n",
       "Anesthesia                 01996       178\n",
       "                           99141        23\n",
       "                           99144        19\n",
       "Evaluation and management  99231      1078\n",
       "                           99232      2385\n",
       "                           99233      1924\n",
       "                           99254      1170\n",
       "                           99291      4758\n",
       "Medicine                   90801       238\n",
       "                           90935      1027\n",
       "                           94002      8206\n",
       "                           94003     12871\n",
       "                           99024       169\n",
       "Pathology and laboratory   85060        22\n",
       "Radiology                  75940        32\n",
       "                           75989        19\n",
       "                           76604        57\n",
       "                           76937       352\n",
       "                           76942       447\n",
       "Surgery                    31624       469\n",
       "                           31645       342\n",
       "                           33405       436\n",
       "                           36556      1788\n",
       "                           36620       661\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df_cpt[['SECTIONHEADER','CPT_CD']].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-memorial",
   "metadata": {},
   "source": [
    "# Filter the Data - ICD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-sterling",
   "metadata": {},
   "source": [
    "I filtered the CID codes so there wouldn't be any notes with less than 100 notes. I also filtered to the top 5 ICD-9 codes to increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "spoken-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_icd(df, threshold):\n",
    "\n",
    "    x = df['ICD9_CODE'].value_counts() > threshold\n",
    "    y = list(x[x == 1].index)\n",
    "\n",
    "    z = df[df['ICD9_CODE'].isin(y)]\n",
    "\n",
    "    return z\n",
    "\n",
    "def filter_icd_ct(df,ct):\n",
    "    top_codes = list(df['ICD9_CODE'].value_counts().head(ct).index)\n",
    "    df = df[df.ICD9_CODE.isin(top_codes)]\n",
    "    return df\n",
    "\n",
    "filtered_df_icd = filter_df_icd(note_icd, 100)\n",
    "filtered_df_icd = filter_icd_ct(filtered_df_icd, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "collective-membrane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ICD9_CODE\n",
       "41401        3463\n",
       "0389         1976\n",
       "41071        1719\n",
       "V3001        1390\n",
       "4241         1136\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df_icd[['ICD9_CODE']].value_counts()\n",
    "# filtered_df_icd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-september",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-fitness",
   "metadata": {},
   "source": [
    "This was another step that was completed to preprocess the data. Sections that were not relevant were removed, such as social history, family history, and medication. Other steps that were taken included lowercasing all letters, removing dates and location, replacing return characters, replacing punctuation, and replacing digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unable-minimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "<ipython-input-19-20c3e3db6ba4>:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df_cpt['TEXT'] = clean_data(filtered_df_cpt['TEXT'], True)\n"
     ]
    }
   ],
   "source": [
    "def clean_data(text_series, remove_sections):\n",
    "  \n",
    "    # Remove topics\n",
    "    data = text_series.str.lower() # lowercase all letters\n",
    "    data = data.str.split(r'(\\n\\n)')\n",
    "    if remove_sections:\n",
    "        for row_num, value in enumerate(data):\n",
    "            text_chunks = [x.split(':', maxsplit=1) for x in value]\n",
    "            ls = []\n",
    "            for i, x in enumerate(text_chunks):\n",
    "                try:\n",
    "                    if x[0] != 'social history' or x[0] != 'family history' or 'medication' not in x[0]:\n",
    "                        ls.append(x[1])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            text_series.iloc[row_num] = ' '.join(ls)\n",
    "\n",
    "        \n",
    "    # Remove dates and locations\n",
    "    text_series = text_series.str.replace('\\[\\*\\*(.*?)\\*\\*\\]', ' ', regex=True)\n",
    "    \n",
    "    # Replace \\n \n",
    "    text_series = text_series.str.replace('\\\\n',' ', regex=True)  \n",
    "    \n",
    "    # Replace punctuation\n",
    "    text_series = text_series.str.replace('[' + string.punctuation + ']', ' ', regex=True)\n",
    "    \n",
    "    # Remove all digits\n",
    "    text_series = text_series.str.replace('\\d',' ', regex=True)\n",
    "    \n",
    "    # Replace plurals, endings with ing, endings with ed, endings with ly\n",
    "#     text_series = text_series.str.replace('s(?=\\s)', ' ', regex=True)\n",
    "#     text_series = text_series.str.replace('ing(?=\\s)', ' ', regex=True)\n",
    "#     text_series = text_series.str.replace('ed(?=\\s)', ' ', regex=True)\n",
    "#     text_series = text_series.str.replace('ly(?=\\s)', ' ', regex=True)\n",
    "    \n",
    "    return text_series\n",
    "\n",
    "# Update Text Column -----\n",
    "filtered_df_cpt['TEXT'] = clean_data(filtered_df_cpt['TEXT'], True)\n",
    "filtered_df_icd['TEXT'] = clean_data(filtered_df_icd['TEXT'], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-somewhere",
   "metadata": {},
   "source": [
    "# Select CPT Code Sections to Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-wilderness",
   "metadata": {},
   "source": [
    "Set the variable sections to the list of CPT code sections. Anything not in those sections will have the option to grouped together into a category called 'other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "documented-coordinate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anesthesia', 'Surgery', 'Medicine', 'Radiology', 'Evaluation and management', 'Pathology and laboratory']\n",
      "['Evaluation and management', 'Surgery', 'Medicine', 'Radiology']\n"
     ]
    }
   ],
   "source": [
    "sections = list(set(filtered_df_cpt['SECTIONHEADER']))\n",
    "print(sections)\n",
    "sections = ['Evaluation and management', 'Surgery', 'Medicine', 'Radiology']\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-brain",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-guinea",
   "metadata": {},
   "source": [
    "Define a function to split the CPT code data. Sets test dataset size to 30% and training to 70%. Also shuffles the data to remove any bias implicit in the data ordering. Finally, combines CPT sections not selected into an other category in order to increase the number of notes in that section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "double-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_dict = {} \n",
    "\n",
    "def split_stratify_df(df, sections, combine_others=False, all_=False):\n",
    "    \n",
    "    # Train test split for each section selected\n",
    "    if all_:\n",
    "        sections.clear()\n",
    "    else:\n",
    "        for i in sections:\n",
    "            \n",
    "            # Test and training\n",
    "            df_x = df[df['SECTIONHEADER'] == i]['TEXT'].values\n",
    "            df_y = df[df['SECTIONHEADER'] == i]['CPT_CD']\n",
    "            tt_dict['X_train_' + i], tt_dict['X_test_' + i], tt_dict['y_train_' + i], tt_dict['y_test_' + i], \\\n",
    "            tt_dict['index_train_' + i], tt_dict['index_test_' + i] = \\\n",
    "            train_test_split(df_x, df_y, range(len(df_y)), test_size = .3, random_state = 42, shuffle=True)\n",
    "            \n",
    "            # Validation\n",
    "            tt_dict['X_test_' + i], tt_dict['X_val_' + i], tt_dict['y_test_' + i], tt_dict['y_val_' + i], \\\n",
    "            tt_dict['index_test_' + i], tt_dict['index_val_' + i] = \\\n",
    "            train_test_split(tt_dict['X_test_' + i], tt_dict['y_test_' + i], range(len(tt_dict['y_test_' + i])), \\\n",
    "                             test_size = .05, random_state = 42, shuffle=True)\n",
    "            \n",
    "    # Group other sections not included in selection\n",
    "    if combine_others:\n",
    "        i = 'other'\n",
    "        other_sections = list(set(df['SECTIONHEADER']).difference(set(sections)))\n",
    "        sections.append(i)\n",
    "        df_x = df[df['SECTIONHEADER'].isin(other_sections)]['TEXT'].values\n",
    "        df_y = df[df['SECTIONHEADER'].isin(other_sections)]['CPT_CD']\n",
    "        \n",
    "        # Test and training\n",
    "        tt_dict['X_train_' + i], tt_dict['X_test_' + i], tt_dict['y_train_' + i], tt_dict['y_test_' + i], \\\n",
    "        tt_dict['index_train_' + i], tt_dict['index_test_' + i] = \\\n",
    "        train_test_split(df_x, df_y, range(len(df_y)), test_size = .3, random_state = 42, shuffle=True)\n",
    "        \n",
    "        # Validation\n",
    "        tt_dict['X_test_' + i], tt_dict['X_val_' + i], tt_dict['y_test_' + i], tt_dict['y_val_' + i], \\\n",
    "        tt_dict['index_test_' + i], tt_dict['index_val_' + i] = \\\n",
    "        train_test_split(tt_dict['X_test_' + i], tt_dict['y_test_' + i], range(len(tt_dict['y_test_' + i])), \\\n",
    "                         test_size = .05, random_state = 42, shuffle=True)\n",
    "        \n",
    "split_stratify_df(filtered_df_cpt, sections, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-witch",
   "metadata": {},
   "source": [
    "# Select ICD sections to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "exciting-circular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Evaluation and management', 'Surgery', 'Medicine', 'Radiology', 'other', 'icd']\n"
     ]
    }
   ],
   "source": [
    "sections.append('icd')\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-adobe",
   "metadata": {},
   "source": [
    "# Split the Data - ICD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "breathing-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test\n",
    "tt_dict['X_train_icd'], tt_dict['X_test_icd'] , tt_dict['y_train_icd'], tt_dict['y_test_icd'] = \\\n",
    "train_test_split(filtered_df_icd['TEXT'].values, filtered_df_icd['ICD9_CODE'], test_size = .3, random_state = 42, shuffle=True)\n",
    "\n",
    "# Test and Validation\n",
    "tt_dict['X_test_icd'], tt_dict['X_val_icd'] , tt_dict['y_test_icd'], tt_dict['y_val_icd'] = \\\n",
    "train_test_split(tt_dict['X_test_icd'], tt_dict['y_test_icd'], test_size = .05, random_state = 42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-removal",
   "metadata": {},
   "source": [
    "# Balance the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-supplement",
   "metadata": {},
   "source": [
    "After splitting the data, the data needs to be balanced due to the dataset imbalance. Datasets are both under and oversampled based on the 99th percentile of the largest CPT/ICD-9 code. This improves the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "quantitative-catalog",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and management\n",
      "New Balanced Record Count per feature: 3281\n",
      "Surgery\n",
      "New Balanced Record Count per feature: 1230\n",
      "Medicine\n",
      "New Balanced Record Count per feature: 8878\n",
      "Radiology\n",
      "New Balanced Record Count per feature: 306\n",
      "other\n",
      "New Balanced Record Count per feature: 122\n",
      "icd\n",
      "New Balanced Record Count per feature: 2397\n"
     ]
    }
   ],
   "source": [
    "def oversample_df(X_train, y_train, percentile):\n",
    "            \n",
    "    # Recombine the training dataset\n",
    "    x = pd.Series(X_train).reset_index(drop=True)\n",
    "    y = pd.Series(y_train).reset_index(drop=True)\n",
    "    training_df = pd.concat([x,y], axis=1, ignore_index=True)\n",
    "\n",
    "    # Check counts\n",
    "    df_cts = training_df.iloc[:,1].value_counts()\n",
    "    record_ct = round(np.percentile(df_cts, percentile))\n",
    "    print('New Balanced Record Count per feature: {}'.format(record_ct))\n",
    "    \n",
    "    # Create a list of CPT values\n",
    "    df = list(df_cts.index.values)\n",
    "\n",
    "    # Resample\n",
    "    minority_df = []\n",
    "    for i in df:\n",
    "        test_resampled = resample(training_df[training_df.iloc[:,1] == i], replace=True, n_samples=record_ct, random_state=123)\n",
    "        minority_df.append(test_resampled)\n",
    "    \n",
    "    # Create final dataframe\n",
    "    new_df = pd.concat(minority_df)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def balance(tt_dict, sections):\n",
    "    for section in sections:\n",
    "        print(section)\n",
    "        \n",
    "        # Running balance function\n",
    "        training_balanced = oversample_df(tt_dict['X_train_' + section], tt_dict['y_train_' + section] , 99)\n",
    "        \n",
    "        # Reassign balanced data\n",
    "        tt_dict['X_train_' + section] = np.array(training_balanced.iloc[:,0].values)\n",
    "        tt_dict['y_train_' + section] = np.array(training_balanced.iloc[:,1].values)\n",
    "\n",
    "# Run the functions\n",
    "balance(tt_dict, sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-comment",
   "metadata": {},
   "source": [
    "# Tokenize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-excess",
   "metadata": {},
   "source": [
    "To run the NLP model, the data needs to be tokenized, which means the words need to be counted for each document. The vectorizer used is the TF-IDF vectorizer. This vectorizer is defined by this equation: TF(t,d) * IDF(t), where t is term frequency, d is the number of times term t appears in a document and IDF stands for the inverse document frequency. IDF is defined as log (1 + n/1 + df9d,t) + 1. N is the number of documents and df is the document frequency of t. There is one vectorizer made for each CPT/ICD-9 section. There are 6 sections total, including the ICD section. The hyperparameters were tuned using grid search in one of the draft notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "determined-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words\n",
    "my_stop_words = list(set(stopwords.words('english'))) \\\n",
    "                + ['admission', 'date', 'sex'] \\\n",
    "                + ['needed', 'every', 'seen', 'weeks', 'please', 'ml', 'unit', 'small', 'year', 'old', 'cm', 'non', 'mm', 'however']\n",
    "                # Got the above from my top 100 most predictive words that I wanted to remove\n",
    "\n",
    "# Set Dictionary for vectorized words\n",
    "vectorized_words = {}\n",
    "    \n",
    "def vectorize_df(train_test_dict, sections):\n",
    "\n",
    "    for section in sections:\n",
    "        \n",
    "        # Import TfidfVectorizer\n",
    "        vectorized_words['tfidf_vectorizer_' + section] = TfidfVectorizer(stop_words=my_stop_words, max_df=.7, min_df = 2, sublinear_tf = True, ngram_range = (1, 2))\n",
    "\n",
    "        # Transform the training data\n",
    "        tfidf_train = vectorized_words['tfidf_vectorizer_' + section].fit_transform(train_test_dict['X_train_' + section])\n",
    "\n",
    "        # Transform the test data\n",
    "        tfidf_test = vectorized_words['tfidf_vectorizer_' + section].transform(train_test_dict['X_test_' + section])\n",
    "\n",
    "        # Add results to dictionary\n",
    "\n",
    "        vectorized_words['tfidf_train_' + section] = tfidf_train\n",
    "        vectorized_words['tfidf_test_' + section] = tfidf_test\n",
    "\n",
    "vectorize_df(tt_dict, sections)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-census",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-tutorial",
   "metadata": {},
   "source": [
    "Logistic regression was chosen after considering 6 ML models in total, these included: Decision Tree, Random Forest, XG Boost, Multinomail Naive Bayes, KNN, and One vs Rest Logistic Regression. The hyperparameters were tuned in another notebook using gridsearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beginning-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amartins\\onedrive - intermountain healthcare\\python_pycharm_virt_env\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models_lr = {}\n",
    "\n",
    "def run_clf(train_test_dict, sections):\n",
    "\n",
    "    # Fit and check accuracy\n",
    "    for section in sections:\n",
    "        \n",
    "        # Use Naive Bayes model\n",
    "        clf = LogisticRegression(random_state=123, C=1, max_iter=25, solver='sag', n_jobs=-1)\n",
    "        \n",
    "        # Fit model\n",
    "        clf.fit(vectorized_words['tfidf_train_' + section], train_test_dict['y_train_' + section])\n",
    "        \n",
    "        # Save in dictionary\n",
    "        models_lr[section] = clf\n",
    "    \n",
    "run_clf(tt_dict, sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-detective",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-application",
   "metadata": {},
   "source": [
    "This function checked the overall accuracy for each of the 6 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "forty-treaty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and management\n",
      "0.3234108527131783\n",
      "Surgery\n",
      "0.597340930674264\n",
      "Medicine\n",
      "0.836502493765586\n",
      "Radiology\n",
      "0.6872586872586872\n",
      "other\n",
      "0.8405797101449275\n",
      "icd\n",
      "0.8891304347826087\n"
     ]
    }
   ],
   "source": [
    "# models_ls = [models_nb, models_rf, models_xg, models_knn, models_lr, models_tree]\n",
    "models_ls = [models_lr]\n",
    "\n",
    "# Check accuracy\n",
    "def predictions(tt_dict, models, section=[]):\n",
    "    for key, model in models.items():\n",
    "        if len(section) == 0 or key == section:\n",
    "            pred = model.predict(vectorized_words['tfidf_test_' + key])\n",
    "            print(key)\n",
    "            print(metrics.accuracy_score(tt_dict['y_test_' + key], pred))\n",
    "\n",
    "predictions(tt_dict, models_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-kenya",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-canada",
   "metadata": {},
   "source": [
    "The classification report can be run for each model to find the precision, recall, and f-1 score to measure the models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "useful-strand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       01996       0.89      0.94      0.92        52\n",
      "       85060       0.80      0.57      0.67         7\n",
      "       99141       0.67      0.50      0.57         4\n",
      "       99144       0.50      0.50      0.50         6\n",
      "\n",
      "    accuracy                           0.84        69\n",
      "   macro avg       0.71      0.63      0.66        69\n",
      "weighted avg       0.83      0.84      0.83        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create classification report taken from here: https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "print('Test')\n",
    "section = 'other'\n",
    "pred = models_lr[section].predict(vectorized_words['tfidf_test_' + section])\n",
    "print(classification_report(tt_dict['y_test_' + section], pred))\n",
    "\n",
    "# print('Training')\n",
    "# pred_x = nb_classifier.predict(vectorized_words['tfidf_train'])\n",
    "# print(classification_report(tt_dict['y_train_' + section], pred_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-mining",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-closure",
   "metadata": {},
   "source": [
    "Predictions can be made using each model. The prediction probability was tested to be used in the front-end of the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "similar-senator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0389'] 96.32%\n",
      "0389\n"
     ]
    }
   ],
   "source": [
    "def predict_cpt(text, section):\n",
    "    input_text_clean = clean_data(pd.Series(text), False)\n",
    "    tfidf_input_test = vectorized_words['tfidf_vectorizer_' + section].transform(input_text_clean)\n",
    "    clf = models_lr[section]\n",
    "    \n",
    "    clf.predict(tfidf_input_test)\n",
    "    print(clf.predict(tfidf_input_test), str(round(max(clf.predict_proba(tfidf_input_test)[0]) * 100,2)) + '%')\n",
    "#     print(clf.predict_proba(tfidf_input_test)[0])\n",
    "\n",
    "# Set Variables\n",
    "txt_record_no = 1\n",
    "section = 'icd'\n",
    "\n",
    "# Run Function\n",
    "predict_cpt(tt_dict['X_val_' + section][txt_record_no], section)\n",
    "\n",
    "# print(np.array(tt_dict['y_val_' + section])[txt_record_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-swing",
   "metadata": {},
   "source": [
    "# Look at the Most/Least Predictive Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-forge",
   "metadata": {},
   "source": [
    "The top 100 and bottom 100 features are saved into the features_df dataframe. These are saved to csv files and later used in the front-end of the app using streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "spoken-adobe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 & Section: Evaluation and management\n",
      "Class 1 & Section: Evaluation and management\n",
      "Class 2 & Section: Evaluation and management\n",
      "Class 3 & Section: Evaluation and management\n",
      "Class 4 & Section: Evaluation and management\n",
      "Class 0 & Section: Surgery\n",
      "Class 1 & Section: Surgery\n",
      "Class 2 & Section: Surgery\n",
      "Class 3 & Section: Surgery\n",
      "Class 4 & Section: Surgery\n",
      "Class 0 & Section: Medicine\n",
      "Class 1 & Section: Medicine\n",
      "Class 2 & Section: Medicine\n",
      "Class 3 & Section: Medicine\n",
      "Class 4 & Section: Medicine\n",
      "Class 0 & Section: Radiology\n",
      "Class 1 & Section: Radiology\n",
      "Class 2 & Section: Radiology\n",
      "Class 3 & Section: Radiology\n",
      "Class 4 & Section: Radiology\n",
      "Class 0 & Section: other\n",
      "Class 1 & Section: other\n",
      "Class 2 & Section: other\n",
      "Class 3 & Section: other\n",
      "Class 0 & Section: icd\n",
      "Class 1 & Section: icd\n",
      "Class 2 & Section: icd\n",
      "Class 3 & Section: icd\n",
      "Class 4 & Section: icd\n"
     ]
    }
   ],
   "source": [
    "def get_features(sections):\n",
    "\n",
    "    # Initialize dataframe list\n",
    "    df_ls = []\n",
    "    \n",
    "    for section in sections: \n",
    "    \n",
    "        # Loop through for each class\n",
    "        for index, class_ in enumerate(models_lr[section].classes_):\n",
    "            \n",
    "            print('Class ' + str(index) + ' & Section: ' + section)\n",
    "\n",
    "            # Get the feature names\n",
    "            feature_names = vectorized_words['tfidf_vectorizer_' + section].get_feature_names()\n",
    "\n",
    "            # Get the probabilities\n",
    "            # Source: # https://sebastiansauer.github.io/convert_logit2prob/ for converting odds to log odds\n",
    "            probs = [np.exp(x)/(1 + np.exp(x)) for x in models_lr[section].coef_[index]]\n",
    "\n",
    "            # Zip together the first CPT weights with feature names\n",
    "            feat_with_weights =  sorted(zip(probs, feature_names))\n",
    "            feat_with_weights_r = feat_with_weights[::-1]\n",
    "\n",
    "            # Bottom 100 dataframe\n",
    "            bottom_100 = pd.DataFrame(feat_with_weights[:100], columns = ['Prob','Features'])\n",
    "            bottom_100['Class'] = models_lr[section].classes_[index]\n",
    "            bottom_100['Direction'] = 'Bottom'\n",
    "            bottom_100['Section'] = section\n",
    "\n",
    "            # Top 100 dataframe\n",
    "            top_100 = pd.DataFrame(feat_with_weights_r[:100], columns = ['Prob','Features'])\n",
    "            top_100['Class'] = models_lr[section].classes_[index]\n",
    "            top_100['Direction'] = 'Top'\n",
    "            top_100['Section'] = section\n",
    "\n",
    "            # Add dataframes to list\n",
    "            df_ls.append(bottom_100)\n",
    "            df_ls.append(top_100)\n",
    "\n",
    "    return pd.concat(df_ls)\n",
    "\n",
    "features_df = get_features(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "driven-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features\n",
    "features_df.to_csv('features_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-theology",
   "metadata": {},
   "source": [
    "# Save the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-colleague",
   "metadata": {},
   "source": [
    "Each model is saved for later use in the front-end Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dimensional-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from here: https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "# save the model to disk\n",
    "\n",
    "for key, model in models_lr.items():\n",
    "    filename = 'finalized_model_' + key + '.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-shield",
   "metadata": {},
   "source": [
    "# Save the Fitted Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-bride",
   "metadata": {},
   "source": [
    "The vectorizers are saved for later use in the front-end Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "broad-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, vectorizer in vectorized_words.items():\n",
    "    if 'vectorizer' in key:\n",
    "        filename = 'finalized_vectorizer_' + key + '.sav'\n",
    "        pickle.dump(vectorizer, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-wheel",
   "metadata": {},
   "source": [
    "# Save the Text Sample Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-colombia",
   "metadata": {},
   "source": [
    "The text sample files are saved to use in the Streamlit app to generate examples of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "romantic-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in tt_dict.items():\n",
    "    if 'X_val_' in key or 'y_val_' in key:\n",
    "        pd.Series(value).to_csv(key + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
