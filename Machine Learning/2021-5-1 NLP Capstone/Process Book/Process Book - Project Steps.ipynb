{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "irish-peoples",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-armor",
   "metadata": {},
   "source": [
    "There were 17 versions for the back-end portion of this project and 2 versions of the front-end. All of these drafts are located in the 'drafts' folder in the 'Notebooks' folder. I will summarize the different approaches that I took in each notebook and try to include some sample code below. The sample code here is taken out of the code context and is not meant to run, but it is supposed to make it so you don't have to go into each notebook to see it yourself. \n",
    "\n",
    "Besides the front and back-end iterations, I also have a draft book for connecting to the UMLS API, which I used to retrieve the CPT descriptions used in my web app. There is also a notebook for data exploration. I will do my best to cover the steps I took, but this is just an overview and won't be exhaustive of all the different things that I considered in my project. If each notebook is opened, most of them have changes from the previous notebook written at the top for easy reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-dylan",
   "metadata": {},
   "source": [
    "# Notebook Iterations Explanation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-guinea",
   "metadata": {},
   "source": [
    "I put the accuracy score next to each notebook for quick reference. The accuracy gradually improved with each iteration. I summarize steps taken for each notebook below:\n",
    "1. Imported the data and ran the initial naive bayes model. Filtered the dataset to just discharge summary to improve accuracy. Ran the model on all CPT codes and only got 11% accuracy.\n",
    "2. Limited the clinical notes to just those codes with over 1000 notes\n",
    "3. Limited the model to predict just the 4 most frequent CPT codes\n",
    "4. Added a text cleaning function to improve accuracy\n",
    "5. Compared running the count vectorizer to the tf-idf vectorizer. The tf-idf vectorizer performed better.\n",
    "6. Tried using a lemmitizer, didn't seem to improve the model performance and so I didn't use it. The code sample I used is below\n",
    "7. Label encoded the data and found it doesn't affect performance, but it's good practice\n",
    "8. Did some hyperparameter tuning on the model and vectorizer\n",
    "9. Shuffled data to improve accuracy, updated the stop words\n",
    "10. Limiting CPT codes to the first CPT code assigned\n",
    "11. Tried using sentence tokenization instead of word tokenization\n",
    "12. Tried using UMLS and cTakes preprocessing to train - didn't improve model accuracy\n",
    "13. Cleaned up the folder and the data\n",
    "14. Split out the model by CPT section and ran one model for each one\n",
    "15. Hyperparameter tuning\n",
    "16. Added diagnosis text to clinical text to improve model performance by using more notes - there were only diagnosis notes for two CPT codes\n",
    "17. Combined the CPT and ICD notebooks into one, used grid search for hyperparameter tuning. Example is listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-weapon",
   "metadata": {},
   "source": [
    "# Code Snippet from CPT Notebook Iteration #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens_tag = nltk.pos_tag(tokens)\n",
    "    tokens_tag = [(i[0], get_wordnet_pos(i[1])) for i in tokens_tag]\n",
    "    final_tokens = [lemmatizer.lemmatize(i[0], i[1]) if i[1] != '' else i[0] for i in tokens_tag]\n",
    "    return final_tokens\n",
    "\n",
    "# Run stop words through my tokenizer\n",
    "my_stop_words_updated = [my_tokenizer(i)[0] for i in my_stop_words]\n",
    "\n",
    "# Tokenize the data -----\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=my_stop_words_updated, max_df = .7, tokenizer=my_tokenizer)\n",
    "\n",
    "# Transform the training data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-cartoon",
   "metadata": {},
   "source": [
    "# Code Snippet from Notebook Iteration #17 - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words\n",
    "my_stop_words = list(set(stopwords.words('english'))) \\\n",
    "                + ['admission', 'date', 'sex'] \\\n",
    "                + ['needed', 'every', 'seen', 'weeks', 'please', 'ml', 'unit', 'small', 'year', 'old', 'cm', 'non', 'mm', 'however']\n",
    "                # Got the above from my top 100 most predictive words that I wanted to remove\n",
    "\n",
    "# Taken from: https://stackoverflow.com/questions/44066264/how-to-choose-parameters-in-tfidfvectorizer-in-sklearn-during-unsupervised-clust/44080802\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=my_stop_words)),\n",
    "    ('clf', LogisticRegression(random_state=123)),\n",
    "])\n",
    "parameters = {\n",
    "#     'tfidf__max_df': (.15,.2,.25,.3) # .2 is the best param\n",
    "#     , 'tfidf__sublinear_tf' : (True, False) # True\n",
    "#     'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)], # (1,2) is best\n",
    "#     'clf__alpha': (0.001, .01) # .001 is best\n",
    "    \n",
    "#     'tfidf__max_df': (.2,.5,.7) # .7 is the best param\n",
    "#     , 'tfidf__ngram_range': [(1, 1), (1, 2)] # (1,2) is best\n",
    "#     , 'tfidf__min_df': (1,2,3) # 2 is the best\n",
    "#     , 'clf__alpha': (0.001, .01, .1,.5) # .001 is best\n",
    "    'clf__C': (.8,.9,1) # 1\n",
    "#     , 'clf__solver': ('liblinear', 'sag', 'saga') # sag\n",
    "    , 'clf__max_iter': (25,40,50) # 25\n",
    "   \n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=-1, verbose=3, scoring='accuracy')\n",
    "grid_search_tune.fit(tt_dict['X_train_other'], tt_dict['y_train_other'])\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
